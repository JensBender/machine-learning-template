{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf7c4ae-408e-4aeb-9fe9-e20263d2ac7f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f512f9-2618-4b1a-a361-69b63dcfdcd7",
   "metadata": {},
   "source": [
    "**Overview:** Brief description of the problem, the dataset, and the main objectives of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9288e0-3815-4c47-94c3-33d0a6aafc8e",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00680c12-3986-4262-9d6d-e330b55fd079",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100528be-33fb-4f95-b8f8-e791de38fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896aa22-da27-4d06-b34f-d63ceace4ff0",
   "metadata": {},
   "source": [
    "## Environment Variables \n",
    "**Note**: Setting environment variables is optional, but it is recommended if you store sensitive information (such as API keys or database credentials) in a `.env` file. Using environment variables helps keep such information secure and separate from your codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bb3ec-8e57-460d-8b31-0508025f19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from .env \n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Get SQL database credentials from .env\n",
    "sql_username = os.getenv(\"SQL_USERNAME\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93f5c7-8968-4a0e-907f-a6bc60d0dd3d",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de96aef-7438-41fd-bd1d-8cdbdc0ed9cb",
   "metadata": {},
   "source": [
    "## CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d0cf0-e26b-4ab4-9070-cf96d67e5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a csv file into a Pandas DataFrame\n",
    "df = pd.read_csv(\"your_csv_file_here.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63482d3-000f-4235-a16a-51d6d8a08dba",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81da48-0441-4908-ba1a-990449b65ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database info\n",
    "mysql_host = \"localhost\"  # Default hostname for a MySQL server running locally\n",
    "mysql_port = 3306  # Default port for MySQL\n",
    "mysql_database_name = \"your_mysql_database_name_here\"\n",
    "mysql_table_name = \"your_mysql_table_name_here\"\n",
    "\n",
    "# Create an SQLAlchemy engine for interacting with the MySQL database\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{sql_username}:{sql_password}@{mysql_host}:{mysql_port}/{mysql_database_name}\")\n",
    "\n",
    "# Load data from MySQL database into a Pandas DataFrame\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {mysql_table_name}\", con=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7809f-dd55-4350-a52d-7c20c761a102",
   "metadata": {},
   "source": [
    "# Initial Data Inspection  \n",
    "Basic exploration of the dataset to understand its structure and detect obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d22f7-282d-47ee-bfcf-e85352c450c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info to check the number of rows and columns, data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00719b-3f66-405d-97a0-79907e91ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows to get a sense of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad1422-e239-4d5d-b6e4-31661d8050be",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f1a0d-a98f-4446-8d44-d7e553e6230f",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee509a36-86a0-4e56-bbeb-ecb29a809288",
   "metadata": {},
   "source": [
    "Duplicates based on all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71162a8d-4f29-470c-a919-8494ef16e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose duplicates \n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0253293-b7c9-4335-9eef-61b29a09ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f43e8-f544-414c-8979-d3a52b0b919d",
   "metadata": {},
   "source": [
    "Duplicates based on specific columns, e.g. the ID column or a combination of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec11f9-42d4-4bd2-a769-c6affd2778d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose duplicates\n",
    "df.duplicated(subset=[\"column_1\", \"column_2\", \"column_3\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620ab52-9ff0-4b3d-a0d6-ee856fdfb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=[\"column_1\", \"column_2\", \"column_3\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28c409-34a3-493f-8cd9-856b5c25e0d3",
   "metadata": {},
   "source": [
    "## Handling Incorrect Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92d718-ec5a-491c-873b-f2b8230f71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from str to int\n",
    "df[\"int_column\"] = df[\"str_column\"].astype(\"Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb41a-a715-4c64-a6ca-9db164c4dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from str to datetime\n",
    "df[\"datetime_column\"] = pd.to_datetime(df[\"str_column\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff6c41-2244-462e-968a-9e8473a89ed6",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9931c-d4f6-4255-a894-2be60a02c26e",
   "metadata": {},
   "source": [
    "### Categorical from String  \n",
    "Extract a categorical feature from a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639472a6-9633-4972-b19f-e57b10ad2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a category from a string   \n",
    "def extract_category_from_string(string):\n",
    "    # Map categories to their corresponding list of keywords\n",
    "    category_keywords_map = {\n",
    "        \"Category 1\": [\"Keyword 1\", \"Keyword 2\", \"Keyword 3\"],\n",
    "        \"Category 2\": [\"Keyword 4\", \"Keyword 5\", \"Keyword 6\"],\n",
    "        \"Category 3\": [\"Keyword 7\", \"Keyword 8\", \"Keyword 9\"]\n",
    "    }\n",
    "\n",
    "    # Loop through each category and its associated keywords \n",
    "    for category, keywords in category_keywords_map.items():\n",
    "        # Check if any keyword is present in the string\n",
    "        if any(keyword in string for keyword in keywords):\n",
    "            return category  # Return the category corresponding to the keyword\n",
    "    return np.nan  # Return a missing value if no keyword matches\n",
    "\n",
    "# Apply function on an existing string column to create a new categorical feature column\n",
    "df[\"categorical_feature\"] = df[\"str_column\"].apply(extract_category_from_string)\n",
    "\n",
    "# Show category frequencies\n",
    "print(df[\"categorical_feature\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9089b-a2fd-4305-aa23-086332d45c01",
   "metadata": {},
   "source": [
    "### Numerical from String  \n",
    "Extract a numerical feature from a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41134f56-1d30-435d-9f32-6ff428d940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "# Function to extract the first number in a string \n",
    "def extract_number_from_string(string):\n",
    "    first_number = re.search(r\"\\b-?\\d+([.,]\\d+)?\\b\", string)  # searches for first integer or float (positive or negative; decimal separator \".\" or \",\")\n",
    "    if first_number:\n",
    "        return float(first_number.group().replace(\",\", \".\"))  # Replace \",\" with \".\" as decimal separator  \n",
    "    else:\n",
    "        return np.nan  # Return a missing value if no number in string\n",
    "\n",
    "# Apply function on an existing string column to create a new numerical feature column\n",
    "df[\"numerical_feature\"] = df[\"str_column\"].apply(extract_number_from_string)\n",
    "\n",
    "# Show descriptive statistics of numerical feature\n",
    "df[\"numerical_feature\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba6942-83b2-4bd4-9223-94bfc963beef",
   "metadata": {},
   "source": [
    "### Boolean from String  \n",
    "Extract a boolean feature from a string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5277ca-3ef2-4e5f-8b06-e2701b601f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords to determine if the feature is present or absent\n",
    "keywords = [\"keyword 1\", \"keyword 2\", \"keyword 3\"]\n",
    "\n",
    "# Extract boolean feature column: True if any keyword is found in the string column\n",
    "df[\"boolean_feature\"] = df[\"str_column\"].apply(lambda x: any(keyword.lower() in x.lower() for keyword in keywords))\n",
    "\n",
    "# Show frequencies of boolean feature\n",
    "print(df[\"boolean_feature\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4d1a9-db9b-4e28-ba65-986831dd8b38",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7806d-1465-4747-8780-b2e28d51a468",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c06ef-bbbf-4d18-bde4-e7a50fc1a1a3",
   "metadata": {},
   "source": [
    "Imputation for numerical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680d14e-5853-4c3f-ac50-d639b7033fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical column\n",
    "df[\"numerical_column\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ca973-3ef1-43a3-b690-69c0ed2f0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the median\n",
    "median = df[\"numerical_column\"].median()\n",
    "df[\"numerical_column\"] = df[\"numerical_column\"].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018d4b9-c440-4b15-aafb-a4bb3a3e79fa",
   "metadata": {},
   "source": [
    "Imputation for categorical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638cab2-bc30-4ae3-9cc1-310b50d5cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of categorical column\n",
    "df[\"categorical_column\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36266f3-6460-48cf-83af-0c888432655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the mode \n",
    "mode = df[\"categorical_column\"].mode()[0]\n",
    "df[\"categorical_column\"] = df[\"categorical_column\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9519b6-a492-4308-abc0-89a470543652",
   "metadata": {},
   "source": [
    "### Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc408e-1e71-4ec1-a678-9b14e16878ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where any column has a missing value \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf359f7-2519-47cf-bb29-60d64918626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where either column_1 or column_2 has a missing value \n",
    "df.dropna(subset=[\"column_1\", \"column_2\"], how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e082e3-b3ab-4c7b-932b-339610614132",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5625577-0d32-402f-bac1-1d4184d765c6",
   "metadata": {},
   "source": [
    "### Remove with 3SD Method  \n",
    "Remove univariate outliers from a numerical column by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2815589-353e-48a4-8941-4a56b037e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_column):\n",
    "        # Calculate mean, standard deviation, and cutoff values of the numerical column\n",
    "        self.mean_ = df[numerical_column].mean()\n",
    "        self.sd_ = df[numerical_column].std()\n",
    "        self.lower_cutoff_ = self.mean_ - 3 * self.sd_\n",
    "        self.upper_cutoff_ = self.mean_ + 3 * self.sd_\n",
    "\n",
    "        # Create a mask for filtering outliers\n",
    "        self.mask_ = (df[numerical_column] >= self.lower_cutoff_) & (df[numerical_column] <= self.upper_cutoff_)\n",
    "\n",
    "        # Show cutoff values and number of outliers\n",
    "        print(f\"Lower cutoff for {numerical_column}: {self.lower_cutoff_}\")\n",
    "        print(f\"Upper cutoff for {numerical_column}: {self.upper_cutoff_}\")\n",
    "        print(f\"{df.shape[0] - df[self.mask_].shape[0]} outliers removed based on {numerical_column} using the 3SD method.\")\n",
    "  \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the mask \n",
    "        return df[self.mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_column):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_column).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d2263-e9de-4b53-8ada-61e1cf8433b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Remove outliers\n",
    "df = outlier_remover_3sd.fit_transform(df, \"numerical_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a030d86-a4da-4b67-bd83-11dcc2af33ec",
   "metadata": {},
   "source": [
    "### Remove with 1.5 IQR Method  \n",
    "Remove univariate outliers from a numerical column using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83870620-24da-4801-9520-ed1a3650a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_column):\n",
    "        # Calculate quartiles, IQR and cutoff values \n",
    "        Q1 = df[numerical_column].quantile(0.25)\n",
    "        Q3 = df[numerical_column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        self.lower_cutoff_ = Q1 - 1.5 * IQR\n",
    "        self.upper_cutoff_ = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create a mask for filtering outliers\n",
    "        self.mask_ = (df[numerical_column] >= self.lower_cutoff_) & (df[numerical_column] <= self.upper_cutoff_)\n",
    "  \n",
    "        # Show cutoff values and number of outliers\n",
    "        print(f\"Lower cutoff for {numerical_column}: {self.lower_cutoff_}\")\n",
    "        print(f\"Upper cutoff for {numerical_column}: {self.upper_cutoff_}\")\n",
    "        print(f\"{df.shape[0] - df[self.mask_].shape[0]} outliers removed based on {numerical_column} using the 1.5 IQR method.\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the mask \n",
    "        return df[self.mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_column):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_column).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798816c-e967-4cd5-a68d-a1bbe2028057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Remove outliers\n",
    "df = outlier_remover_iqr.fit_transform(df, \"numerical_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d48183-607c-4d2b-a462-7d22b0ee0fc4",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fa5b4-8784-4081-9de8-dc83de39533d",
   "metadata": {},
   "source": [
    "### CSV  \n",
    "Save preprocessed data from a Pandas DataFrame to a `csv` file in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d25a24-5dc0-40a4-a022-874a720e9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save as csv  \n",
    "df.to_csv(\"data/preprocessed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bad71b-409a-405e-945d-a0b3a30bbebf",
   "metadata": {},
   "source": [
    "### MySQL  \n",
    "Save preprocessed data from a Pandas DataFrame to a MySQL table.  \n",
    "Note: Make sure `sql_username` and `sql_password` were imported as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f078ca-246c-4ce0-b00e-515db9e916e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database info\n",
    "mysql_host = \"localhost\"  # Default hostname for a MySQL server running locally\n",
    "mysql_port = 3306  # Default port for MySQL\n",
    "mysql_database_name = \"your_mysql_database_name_here\"\n",
    "mysql_table_name = \"preprocessed_data\"\n",
    "\n",
    "try:\n",
    "    # Create an SQLAlchemy engine for interacting with the MySQL database\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{sql_username}:{sql_password}@{mysql_host}:{mysql_port}/{mysql_database_name}\")\n",
    "    \n",
    "    # Save data to MySQL \n",
    "    with engine.connect() as connection:\n",
    "        df.to_sql(name=mysql_table_name, con=connection, if_exists=\"replace\", index=False)\n",
    "    \n",
    "    print(\"Preprocessed data successfully saved to MySQL.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving preprocessed data to MySQL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca10b19-02a9-4a0f-b543-7374f677d139",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571beab2-bbb5-4fba-8143-69b04a5ea2c8",
   "metadata": {},
   "source": [
    "## Defining Column Types  \n",
    "Define numerical, categorical and boolean columns for downstream tasks like exploratory data analysis, additional preprocessing steps, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f5252-3567-42a7-8ef2-13f52c6a4ec6",
   "metadata": {},
   "source": [
    "### Manual  \n",
    "Option 1: Use the manual approach for small datasets or when you have specific requirements and need precise control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d48ac1-c330-43f6-b460-bf5b57c9acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns and their pandas data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6838b9e-f03b-4791-abd6-ab1c0565e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column types manually\n",
    "numerical_columns = [\"numerical_column_1\", \"numerical_column_2\", \"numerical_column_3\"]\n",
    "categorical_columns = [\"categorical_column_1\", \"categorical_column_2\", \"categorical_column_3\"]\n",
    "boolean_columns = [\"boolean_column_1\", \"boolean_column_2\", \"boolean_column_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c7d33-ac8c-4385-8d16-8bdab200eeed",
   "metadata": {},
   "source": [
    "### Programmatic  \n",
    "Option 2: Use the programmatic approach for large datasets or when you need to automate the process, ensuring efficiency and scalability in handling column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce92eb-c2b8-4fcf-83c6-1ae149b0a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column types programmatically based on pandas data types\n",
    "numerical_columns = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "boolean_columns = df.select_dtypes(include=[\"bool\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b0e23-3001-433d-a0fe-9d3d0c040fe6",
   "metadata": {},
   "source": [
    "## Univariate EDA  \n",
    "Univariate exploratory data analysis (EDA) analyzes the distribution, central tendency, and variability of a single column using descriptive statistics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb6cb0-f661-4be4-b0b5-bb6f0ba46fee",
   "metadata": {},
   "source": [
    "### Numerical Columns  \n",
    "Perform univariate EDA on numerical columns by examining descriptive statistics (such as mean, median, standard deviation and quartiles) and histograms for distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe81ae6-559e-432b-8cf2-8643b13ef6dd",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1f2c8-3fa9-4efb-b766-7c5d3621b789",
   "metadata": {},
   "source": [
    "Single Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715a025-45ee-4ee8-bfff-37ff19ceddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of a single numerical column\n",
    "df[\"numerical_column\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ceaf9-308a-46d6-a0a4-dd81b796bdd6",
   "metadata": {},
   "source": [
    "Multiple Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ce7fc-350e-4624-949a-60d1386bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with descriptive statistics of all numerical columns\n",
    "df[numerical_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76c43e-ff1c-4477-8610-a103373b9383",
   "metadata": {},
   "source": [
    "#### Histogram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c4705-ccfa-4b08-9789-9c5319d71e52",
   "metadata": {},
   "source": [
    "Single Column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcf592-a21f-4155-8d27-b16cffd8cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of numerical column\n",
    "sns.histplot(df[\"numerical_column\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Distribution of numerical_column\")\n",
    "plt.xlabel(\"numerical_column\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f9891-a8ab-4a10-9351-2be2e20a1e8e",
   "metadata": {},
   "source": [
    "### Categorical Columns  \n",
    "Perform univariate EDA on categorical columns by examining descriptive statistics (such as absolute and relative frequencies) and bar plots of frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bdb25-d466-48d1-bf2f-f2168e7bd774",
   "metadata": {},
   "source": [
    "#### Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735f52c-eb16-4658-a566-0fcebf883180",
   "metadata": {},
   "source": [
    "Single Column:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9fe5a-ecf0-4388-8a9d-88590fa48f89",
   "metadata": {},
   "source": [
    "Absolute and relative frequencies of a single categorical colum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a462-d369-4340-97e6-51480f28e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df[\"categorical_column\"].value_counts()\n",
    "relative_frequencies = absolute_frequencies / absolute_frequencies.sum() * 100\n",
    "\n",
    "# Show frequencies\n",
    "print(f\"Absolute frequencies:\\n {absolute_frequencies}\\n\")\n",
    "print(f\"Relative frequencies:\\n {relative_frequencies.round(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057fa22-fe17-4ea5-8558-cd8454ac77d6",
   "metadata": {},
   "source": [
    "Bar plot of frequencies of a single categorical colum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a30df-8cbe-42b6-925d-57d808f9099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "sns.barplot(x=absolute_frequencies.index, y=absolute_frequencies.values, palette=\"colorblind\")\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"categorical_column\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Rotate x-axis tick labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44f3be-22bb-498c-9560-8a53cbe51cee",
   "metadata": {},
   "source": [
    "Multiple Columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9fb2d-babe-4c40-a375-d697bc98ee5f",
   "metadata": {},
   "source": [
    "Absolute and relative frequencies of multiple categorical colums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4b6e5-237c-47df-8d5d-862bd031ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store frequencies for multiple categorical columns\n",
    "absolute_frequencies_dict = {}\n",
    "relative_frequencies_dict = {}\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for categorical_column in categorical_columns:\n",
    "    # Calculate absolute and relative frequencies \n",
    "    absolute_frequencies = df[categorical_column].value_counts()\n",
    "    relative_frequencies = absolute_frequencies / absolute_frequencies.sum() * 100\n",
    "\n",
    "    # Store frequencies\n",
    "    absolute_frequencies_dict[categorical_column] = absolute_frequencies\n",
    "    relative_frequencies_dict[categorical_column] = relative_frequencies\n",
    "\n",
    "    # Show frequencies\n",
    "    print(categorical_column.upper())\n",
    "    print(f\"Absolute frequencies:\\n {absolute_frequencies}\\n\")\n",
    "    print(f\"Relative frequencies (%):\\n {relative_frequencies.round(1)}\")\n",
    "    print(\"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3edff6-44e4-4e07-abd7-38d1fcb178cf",
   "metadata": {},
   "source": [
    "Bar plot matrix that shows frequencies for multiple categorical columns.  \n",
    "Example code for 5 bar plots in a 2x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f74b5-524c-4517-97bb-af5d7a401c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for i, categorical_column in enumerate(categorical_columns):\n",
    "    # Create a subplot in a 2x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Calculate frequencies for the current column\n",
    "    absolute_frequencies = df[categorical_column].value_counts()\n",
    "    \n",
    "    # Create bar plot for the current column\n",
    "    sns.barplot(x=absolute_frequencies.index, y=absolute_frequencies.values, estimator=np.median, ci=None)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(categorical_column.title())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Rotate x-axis tick labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895c96c-daff-4873-852d-615627e17dcb",
   "metadata": {},
   "source": [
    "## Bivariate EDA  \n",
    "Bivariate exploratory data analysis (EDA) examines relationships between two columns using methods such as correlation analysis, scatterplots, group-wise statistics, and bar plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a539fa8d-ba5c-42e9-bf4c-7e178a87ff1c",
   "metadata": {},
   "source": [
    "### Numerical with Numerical  \n",
    "Analyze relationships between two numerical columns using correlation methods and visualizations like scatterplots. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bed553-89be-4154-9775-d784adfc100f",
   "metadata": {},
   "source": [
    "#### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedb34a-4995-4e6d-9024-b526c26002a8",
   "metadata": {},
   "source": [
    "Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a94e3f-694d-4b05-9436-fbeaf82f748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numerical and boolean columns for correlation analysis\n",
    "combined_columns = numerical_columns + boolean_columns\n",
    "\n",
    "# Calculate the correlation matrix \n",
    "correlation_matrix = df[combined_columns].corr()\n",
    "\n",
    "# Round correlations to 2 decimals\n",
    "correlation_matrix = round(correlation_matrix, 2)\n",
    "\n",
    "# Create an upper triangle mask (k=1 excludes the diagonal)\n",
    "mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "\n",
    "# Set upper triangle to NaN to avoid redundant information\n",
    "correlation_matrix[mask] = np.nan\n",
    "\n",
    "# Show correlation matrix\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83902fe5-8ac2-4f64-bf9b-33e84fefb781",
   "metadata": {},
   "source": [
    "Correlation heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6eda3-d6aa-4578-8f55-d80d3e7153aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    cmap=\"viridis\",  # Color map choice\n",
    "    annot=True,  # Show numbers\n",
    "    linewidth=0.5  # Thin white lines between cells\n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92936fd-c3a7-4441-83b1-7bca48db2323",
   "metadata": {},
   "source": [
    "Save correlation heatmap as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8b750-799b-4b7b-8ba4-7cb2cafc9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create \"images\" directory if it doesn't exist\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Save the heatmap as a .png image\n",
    "try:\n",
    "    # Construct full file path\n",
    "    image_path = os.path.join(\"images\", \"correlation_heatmap.png\") \n",
    "    \n",
    "    # Save the heatmap  \n",
    "    plt.savefig(\n",
    "        image_path, \n",
    "        bbox_inches=\"tight\",  # removes unnecessary whitespace\n",
    "        dpi=300  # higher image quality\n",
    "    )\n",
    "    print(f\"Correlation heatmap saved successfully to {image_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving correlation heatmap: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382adcc-1fdb-48b7-8f2a-6d27a559bbfc",
   "metadata": {},
   "source": [
    "Correlations of target variable with features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675dca5-3693-4669-8da3-6d97648bc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between the numerical target variable and each numerical and boolean feature\n",
    "combined_columns = numerical_columns + boolean_columns\n",
    "df[combined_columns].corr()[\"numerical_target\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54741433-1f70-4248-982e-318bba474e15",
   "metadata": {},
   "source": [
    "#### Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94309ab-686d-4390-bb41-1d43b728fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create the scatterplot \n",
    "sns.scatterplot(data=df, x=\"numerical_column_1\", y=\"numerical_column_2\")\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"Relationship between numerical_column_1 and numerical_column_2\")\n",
    "plt.xlabel(\"numerical_column_1\")\n",
    "plt.ylabel(\"numerical_column_2\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c0cc0-34f1-4f4c-b8d0-2243ce6f4e4d",
   "metadata": {},
   "source": [
    "#### Scatterplot Matrix \n",
    "Scatterplot matrix that shows scatterplots between a numerical column (e.g., the target variable) and each other numerical column.   \n",
    "Example code for 9 scatterplots in a 3x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4a478-acd0-4f18-99f8-81f9247250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size \n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# List of numerical columns excluding the target column\n",
    "numerical_columns_without_target = [col for col in numerical_columns if col != \"numerical_target\"]\n",
    "\n",
    "# Iterate over the numerical columns\n",
    "for i, numerical_column in enumerate(numerical_columns_without_target):\n",
    "    # Create a subplot in a 3x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    \n",
    "    # Create a scatterplot between the current column and the numerical target\n",
    "    sns.scatterplot(data=df, x=numerical_column, y=\"numerical_target\")\n",
    "    \n",
    "    # Add title and axis labels \n",
    "    plt.title(f\"numerical_target by {numerical_column}\")\n",
    "    plt.xlabel(f\"{numerical_column}\")\n",
    "    plt.ylabel(\"numerical_target\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdbe81-6e4d-48b4-ae69-07336924ce94",
   "metadata": {},
   "source": [
    "### Numerical with Categorical  \n",
    "Examine relationships between a numerical column and a categorical column using group-wise statistics (such as median or mean by category), and visualizations like bar plots to compare statistics by category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c58bee-b56d-4f5c-9b3f-ccf5cd47203c",
   "metadata": {},
   "source": [
    "#### Group-Wise Statistics \n",
    "Descriptive statistics of a numerical column grouped by categories in a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5d314-45ef-4eee-9cfa-918a52081ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical column by categorical column\n",
    "numerical_by_category = df[\"numerical_column\"].groupby(df[\"categorical_column\"])\n",
    "numerical_by_category.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0cd54-3004-408b-a1b6-4874e034fd71",
   "metadata": {},
   "source": [
    "#### Bar Plot  \n",
    "Bar plot of the median of a numerical column by a single categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6c9f6-896f-4490-a125-3fe8ce91cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the median of the numerical column by category\n",
    "median_by_category = numerical_by_category.median()\n",
    "\n",
    "# Bar plot of median by category\n",
    "sns.barplot(x=median_by_category.index, y=median_by_category.values, palette=\"colorblind\")\n",
    "\n",
    "# Add title and axes labels\n",
    "plt.title(\"Median numerical_column by categorical_column\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"numerical_column\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec66e0-2f7f-49c4-847b-f7b696217640",
   "metadata": {},
   "source": [
    "#### Bar Plot Matrix  \n",
    "Bar plot matrix that shows bar plots between a single numerical column (e.g., the target variable) and each categorical column.  \n",
    "Example code for 5 bar plots in a 2x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6f05a-e6d8-4f98-9046-f2ad3cd56b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size to 12x6 inches\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for i, categorical_column in enumerate(categorical_columns):\n",
    "    # Create a subplot in a 2x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Create a bar plot of the median of the numerical column by the current categorical column\n",
    "    ax = sns.barplot(data=df, x=categorical_column, y=\"numerical_column\", estimator=np.median, ci=None)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(f\"Median numerical_column by {categorical_column}\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"numerical_column\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4d486-4d2c-4214-a260-ed19c446c41b",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split  \n",
    "The ideal split depends on the dataset size and the task. A general guideline is:\n",
    "\n",
    "| Dataset Size                | Training Set | Validation Set | Test Set |\n",
    "|:-----------------------------|--------------|----------------|----------|\n",
    "| Smaller datasets (<10,000)   | 70%          | 15%            | 15%      |\n",
    "| Larger datasets (≥10,000)    | 80%          | 10%            | 10%      | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c905a81-4c0d-4acf-85cd-04f81059d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7fc1d-be3a-4623-9e16-aac8e85d3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (70% train, 30% temporary)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary data into validation and test sets (50% each)\n",
    "# Note: This accomplishes a 70% training, 15% validation and 15% test set size\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Delete the temporary data to free up memory\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54633ff2-9a06-496b-923a-dd8f0f5a7ff3",
   "metadata": {},
   "source": [
    "# Feature Scaling and Encoding  \n",
    "Use a `ColumnTransformer` to preprocess columns based on their data type. This allows the appropriate transformation to each column type in a single pipeline:  \n",
    "- Scale numerical columns: Use `StandardScaler` (scales features to have mean = 0 and standard deviation = 1) or optionally `MinMaxScaler` (scales features to a range between [0, 1]).\n",
    "- Encode categorical columns: Apply `OneHotEncoder` to convert categorical variables into binary (one-hot) encoded vectors.\n",
    "- Pass through boolean columns unchanged: Retain boolean features in their original form using `remainder=\"passthrough\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a284ea-8d0c-463e-9c05-fdb054c18082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a column transformer \n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", StandardScaler(), numerical_columns),  # Use MinMaxScaler() for min-max normalization\n",
    "        (\"encoder\", OneHotEncoder(drop=\"first\"), categorical_columns)  \n",
    "    ],\n",
    "    remainder=\"passthrough\" \n",
    ")\n",
    "\n",
    "# Fit the column transformer to the training data and apply transformations\n",
    "X_train_transformed = column_transformer.fit_transform(X_train)\n",
    "\n",
    "# Apply the same transformations to the validation and test data\n",
    "X_val_transformed = column_transformer.transform(X_val)\n",
    "X_test_transformed = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b7d67-27ed-4716-8c3f-079e2d239339",
   "metadata": {},
   "source": [
    "# Modeling (Regression Task)  \n",
    "For a regression problem, where the goal is to predict a numerical target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a506f-0b4a-4aad-bfac-3e68f65f3dea",
   "metadata": {},
   "source": [
    "## Helper Function for Residual Plots  \n",
    "Creates two plots for predicted vs. actual target and residuals vs. actual target to evaluate model performance and identify potential issues in the model assumptions graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb007a-2d89-431e-ab84-60337886ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create residual plots\n",
    "def plot_residuals(y, y_pred):\n",
    "    # Calculate residuals\n",
    "    residuals = [actual_value - predicted_value for actual_value, predicted_value in zip(y, y_pred)]\n",
    "\n",
    "    # Create a 1x2 grid of subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=150)\n",
    "\n",
    "    # Plot 1: Predicted vs. Actual Target\n",
    "    axes[0].scatter(y, y_pred)\n",
    "    axes[0].plot([min(y), max(y)], \n",
    "                 [min(y), max(y)], \n",
    "                 color=\"red\", \n",
    "                 linestyle=\"--\", \n",
    "                 label=\"Perfect Prediction\")  # Add diagonal reference line\n",
    "    axes[0].set_title(\"Predicted vs. Actual Target\")\n",
    "    axes[0].set_xlabel(\"Actual Target\")\n",
    "    axes[0].set_ylabel(\"Predicted Target\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend() \n",
    "\n",
    "    # Plot 2: Residuals vs. Actual Target\n",
    "    axes[1].scatter(y, residuals)\n",
    "    axes[1].axhline(y=0, color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")  # Add horizontal reference line\n",
    "    axes[1].set_xlabel(\"Actual Target\")\n",
    "    axes[1].set_ylabel(\"Residuals\")\n",
    "    axes[1].set_title(\"Residuals vs. Actual Target\")\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend() \n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21e2d4-cbd4-42f7-b7d1-f3997b3619f1",
   "metadata": {},
   "source": [
    "## Training Baseline Models (Individually)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb12588e-68c3-4fe3-b858-8ab0fb7b800f",
   "metadata": {},
   "source": [
    "### Linear Regression  \n",
    "Hyperparameters and Default Values:\n",
    "- `fit_intercept=True`: Calculates the intercept; can be set to `False` if data is already centered.\n",
    "- `n_jobs=None`: Number of CPU threads; use `-1` for all available processors.\n",
    "- `positive=False`: Forces regression coefficients to be non-negative if set to `True`.\n",
    "\n",
    "For more details, refer to the official [scikit-learn LinearRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e64a3-d589-4f67-9fc3-04d0fc6b78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "lr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = lr.predict(X_train_transformed)\n",
    "y_val_pred = lr.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "lr_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(lr_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da407ee-333e-4bbf-9d83-6696852c8c35",
   "metadata": {},
   "source": [
    "### Support Vector Regressor  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:\n",
    "    - `C=1.0`: Regularization parameter balancing error reduction and model complexity.\n",
    "    - `epsilon=0.1`: Margin of tolerance for predictions without penalty.\n",
    "- Kernel Configuration:\n",
    "    - `kernel=\"rbf\"`: Kernel function for mapping data to higher dimensions (default is radial basis function or \"rbf\"). \n",
    "    - `degree=3`: Degree of the polynomial kernel function (ignored by the rbf kernel). \n",
    "    - `gamma=\"scale\"`: Influence range of a single training example. `\"scale\"` means `1 / (n_features * X.var())`.\n",
    "    - `coef0=0.0`: Independent term in polynomial and sigmoid kernel function (ignored by the rbf kernel).\n",
    "- Training Behavior:\n",
    "    - `tol=1e-3`: Stopping criterion for optimization. If the change in the cost function is less than this tolerance, training stops.\n",
    "    - `cache_size=200`: Memory (MB) allocated for kernel computation. Larger values speed up training.\n",
    "    - `shrinking=True`: Enables the shrinking heuristic, which can speed up training by eliminating unnecessary steps during optimization.\n",
    "    - `verbose=False`: Whether to print progress messages during training.\n",
    "    - `max_iter=-1`: Maximum number of iterations during training (`-1` for no limit).\n",
    "\n",
    "For more details, refer to the official [scikit-learn SVR documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5ef70-af0b-4cc5-8892-21df67f8ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "svr = SVR()\n",
    "\n",
    "# Train model\n",
    "svr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = svr.predict(X_train_transformed)\n",
    "y_val_pred = svr.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "svr_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(svr_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770a103-1db7-4744-b484-44e63c1bf6b3",
   "metadata": {},
   "source": [
    "### Random Forest Regressor  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:\n",
    "  - `n_estimators=100`: Number of trees in the forest.  \n",
    "  - `max_depth=None`: Maximum depth of each tree; `None` allows trees to grow until all leaves are pure or minimum samples are reached.  \n",
    "  - `min_samples_split=2`: Minimum number of samples required to split a node.  \n",
    "  - `min_samples_leaf=1`: Minimum number of samples required at a leaf node.  \n",
    "  - `max_features='auto'`: Number of features considered for the best split; default `auto` uses the square root of all features.  \n",
    "- Regularization:\n",
    "  - `max_leaf_nodes=None`: Maximum number of leaf nodes per tree.  \n",
    "  - `min_impurity_decrease=0.0`: Splits a node only if it decreases impurity by this threshold.  \n",
    "- Training Behavior:\n",
    "  - `bootstrap=True`: Whether to use bootstrap samples for training each tree.  \n",
    "  - `oob_score=False`: Whether to use out-of-bag samples to estimate prediction accuracy.  \n",
    "  - `n_jobs=None`: Number of CPU threads used (`-1` for all processors).  \n",
    "  - `random_state=None`: Random seed for reproducibility.  \n",
    "  - `verbose=0`: Controls the verbosity of output during training.  \n",
    "- Performance Optimization:\n",
    "  - `max_samples=None`: Maximum number of samples used to train each tree, useful for subsampling large datasets.\n",
    " \n",
    "For more details, refer to the official [scikit-learn RandomForestRegressor documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106850a-59f4-4aad-aecb-1b0097c94381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = rf.predict(X_train_transformed)\n",
    "y_val_pred = rf.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "rf_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(rf_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc56c45-c57b-407a-8343-e5217b7d21cf",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Regressor  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Architecture:\n",
    "    - `hidden_layer_sizes=(100,)`: Defines the size and number of hidden layers; `(100,)` indicates one layer with 100 neurons.  \n",
    "    - `activation=\"relu\"`: Activation function for the hidden layers; options include `\"relu\"`, `\"tanh\"`, `\"logistic\"`, or `\"identity\"`.  \n",
    "    - `solver=\"adam\"`: Optimization algorithm; options are `\"adam\"` (default), `\"lbfgs\"`, or `\"sgd\"`.  \n",
    "- Regularization and Learning:  \n",
    "    - `alpha=0.0001`: L2 regularization term to prevent overfitting.  \n",
    "    - `learning_rate=\"constant\"`: Strategy for learning rate adjustment; options are `\"constant\"`, `\"invscaling\"`, or `\"adaptive\"`.  \n",
    "    - `learning_rate_init=0.001`: Initial learning rate for weight updates.  \n",
    "    - `power_t=0.5`: Exponent for inverse scaling of learning rate (used when `learning_rate=\"invscaling\"`).  \n",
    "- Training Behavior:  \n",
    "    - `max_iter=200`: Maximum number of iterations for training.  \n",
    "    - `tol=1e-4`: Tolerance for stopping criteria; training stops if loss improvement is below this value.  \n",
    "    - `momentum=0.9`: Momentum parameter for gradient descent updates (used when `solver=\"sgd\"`).  \n",
    "    - `n_iter_no_change=10`: Number of iterations with no improvement to stop early.  \n",
    "    - `early_stopping=False`: Enables early stopping when validation score doesn’t improve.  \n",
    "- Performance Optimization:  \n",
    "    - `batch_size=\"auto\"`: Number of samples per batch for training; `\"auto\"` uses `min(200, n_samples)`.  \n",
    "    - `shuffle=True`: Whether to shuffle training data before each epoch.  \n",
    "    - `random_state=None`: Random seed for reproducibility.  \n",
    "    - `verbose=False`: Controls verbosity of output during training.  \n",
    "    - `warm_start=False`: Reuses previous solution to initialize weights for additional fitting.  \n",
    "    - `beta_1=0.9`, `beta_2=0.999`: Exponential decay rates for moving averages of gradients and squared gradients (used in `solver=\"adam\"`).  \n",
    "    - `epsilon=1e-8`: Small value to prevent division by zero in `solver=\"adam\"`.\n",
    "\n",
    "For more details, refer to the official [scikit-learn MLPRegressor documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a704b2-b16d-4ad6-84f2-5e0014394140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "mlp.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = mlp.predict(X_train_transformed)\n",
    "y_val_pred = mlp.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "mlp_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(mlp_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d9b1d-e7c0-45fb-a908-008c8c706590",
   "metadata": {},
   "source": [
    "### XGBoost Regressor  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:  \n",
    "    - `n_estimators=100`: Number of trees.  \n",
    "    - `max_depth=6`: Maximum depth of each tree.  \n",
    "    - `learning_rate=0.3`: Step size shrinking to prevent overfitting.  \n",
    "    - `subsample=1.0`: Fraction of training samples used for each tree.  \n",
    "    - `colsample_bytree=1.0`: Fraction of features used for each tree.  \n",
    "    - `colsample_bylevel=1.0`: Fraction of features used at each tree level.  \n",
    "    - `colsample_bynode=1.0`: Fraction of features used at each node.  \n",
    "- Regularization and Learning:  \n",
    "    - `gamma=0`: Minimum loss reduction required to make a further partition on a leaf node.  \n",
    "    - `min_child_weight=1`: Minimum sum of instance weight (hessian) in a child.  \n",
    "    - `scale_pos_weight=1`: Controls the balance of positive and negative weights; used for imbalanced datasets.  \n",
    "- Training Behavior:  \n",
    "    - `objective=\"reg:squarederror\"`: Objective function for regression; default is for squared error.  \n",
    "    - `booster=\"gbtree\"`: Booster type; options include `\"gbtree\"` (default), `\"gblinear\"`, and `\"dart\"`.  \n",
    "    - `tree_method=\"auto\"`: Tree construction algorithm; `\"auto\"` chooses based on system configuration. Options include `\"exact\"`, `\"approx\"`, and `\"hist\"`.  \n",
    "    - `eval_metric=\"rmse\"`: Metric used for validation during training; default is root mean square error (`rmse`).  \n",
    "- Performance Optimization:  \n",
    "    - `early_stopping_rounds=None`: Stops training if validation metric does not improve after specified rounds.  \n",
    "    - `n_jobs=1`: Number of threads used for parallel computation (`-1` for all processors).  \n",
    "    - `random_state=None`: Seed for reproducibility.  \n",
    "    - `verbose=1`: Verbosity level for training output; 0 for silent, higher values show more details.  \n",
    "- Advanced Parameters:  \n",
    "    - `lambda=1`: L2 regularization term on weights.  \n",
    "    - `alpha=0`: L1 regularization term on weights.  \n",
    "    - `max_delta_step=0`: Used to help with convergence in highly imbalanced datasets.\n",
    "\n",
    "For more details, refer to the official [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992fea17-acb8-4e88-8c1a-ce1d3f37758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Train model\n",
    "xgb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = xgb.predict(X_train_transformed)\n",
    "y_val_pred = xgb.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "xgb_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(xgb_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36172487-5eb6-495b-910e-aafdf0365a13",
   "metadata": {},
   "source": [
    "## Training Baseline Models (Pipeline)  \n",
    "Train five baseline models:\n",
    "- Linear Regression\n",
    "- Support Vector Regressor\n",
    "- Random Forest Regressor\n",
    "- Multi-Layer Perceptron Regressor\n",
    "- XGBoost Regressor\n",
    "\n",
    "The model performance will be evaluated using the following metrics:  \n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- R-squared (R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca118a-6a69-4b27-adbc-4bdc77027ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with baseline configurations\n",
    "models = [LinearRegression(), SVR(), RandomForestRegressor(random_state=42), MLPRegressor(random_state=42), XGBRegressor(random_state=42)]\n",
    "\n",
    "# Create lists for storing the evaluation metrics (RMSE, MAPE, R2) of each model \n",
    "rmse_ls = []\n",
    "mape_ls = []\n",
    "r2_ls = []\n",
    "\n",
    "# Loop through each model\n",
    "for model in models:\n",
    "    # Show model\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Scale numerical columns and encode categorical columns \n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"scaler\", StandardScaler(), numerical_columns),\n",
    "            (\"encoder\", OneHotEncoder(drop=\"first\"), categorical_columns)\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Include the boolean columns without transformation\n",
    "    )\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"column_transformer\", column_transformer),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "    # Calculate evaluation metrics: RMSE, MAPE, R2\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    # Show evaluation metrics\n",
    "    print(f\"RMSE: {round(rmse, 2)}\")\n",
    "    print(f\"MAPE: {round(mape, 2)}\")\n",
    "    print(f\"R-squared (R²): {round(r2, 2)}\")\n",
    "\n",
    "    # Add evaluation metrics to their respective lists\n",
    "    rmse_ls.append(rmse)\n",
    "    mape_ls.append(mape)\n",
    "    r2_ls.append(r2)\n",
    "\n",
    "    # Create residual plots\n",
    "    plot_residuals(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bd7a1-73cd-4c7c-bea6-f0de3b7833d8",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning  \n",
    "Based on baseline model training, the following models outperformed the other candidates across evaluation metrics (RMSE, MAPE, R-squared) on the validation data and were selected for hyperparameter tuning:\n",
    "- Model 1 (e.g., Random Forest Regressor)  \n",
    "  *Justification example*: Delivered low RMSE and MAPE scores during baseline evaluation, showing strong predictive performance with minimal error.\n",
    "- Model 2 (e.g., XGBoost Regressor)  \n",
    "  *Justification example*: Achieved high R-squared and consistently low error metrics, indicating its ability to capture underlying patterns in the data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360ea07-5564-47d0-81a7-8203928a18f4",
   "metadata": {},
   "source": [
    "### Random Forest Regressor  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:\n",
    "  - `n_estimators=100`: Number of trees in the forest.  \n",
    "  - `max_depth=None`: Maximum depth of each tree; `None` allows trees to grow until all leaves are pure or minimum samples are reached.  \n",
    "  - `min_samples_split=2`: Minimum number of samples required to split a node.  \n",
    "  - `min_samples_leaf=1`: Minimum number of samples required at a leaf node.  \n",
    "  - `max_features=\"auto\"`: Number of features considered for the best split; default `auto` uses the square root of all features.  \n",
    "- Regularization:\n",
    "  - `max_leaf_nodes=None`: Maximum number of leaf nodes per tree.  \n",
    "  - `min_impurity_decrease=0.0`: Splits a node only if it decreases impurity by this threshold.  \n",
    "- Training Behavior:\n",
    "  - `bootstrap=True`: Whether to use bootstrap samples for training each tree.  \n",
    "  - `oob_score=False`: Whether to use out-of-bag samples to estimate prediction accuracy.  \n",
    "  - `n_jobs=None`: Number of CPU threads used (`-1` for all processors).  \n",
    "  - `random_state=None`: Random seed for reproducibility.  \n",
    "  - `verbose=0`: Controls the verbosity of output during training.  \n",
    "- Performance Optimization:\n",
    "  - `max_samples=None`: Maximum number of samples used to train each tree, useful for subsampling large datasets.\n",
    "\n",
    "For more details, refer to the official [scikit-learn RandomForestRegressor documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor).  \n",
    "\n",
    "The following hyperparameters are typically the most impactful:\n",
    "- `n_estimators`\n",
    "- `max_depth`  \n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `max_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785cd9ad-ec73-41db-81fe-b2b6f4793297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],               \n",
    "    \"max_depth\": [None, 10, 20],              \n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [0.33, 0.66, 1]                \n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf, \n",
    "    param_grid=rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"neg_root_mean_squared_error\"  # use \"r2\" for R-squared or a custom function for MAPE\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "rf_grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601be53-7d15-48ca-9321-6915b90511e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of grid search results \n",
    "rf_grid_search_results = pd.DataFrame({\n",
    "    \"validation_rmse\": -1 * rf_grid_search.cv_results_[\"mean_test_score\"],  # RMSE on validation data\n",
    "    \"parameters\": rf_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each parameter as a separate column\n",
    "rf_grid_search_results[\"n_estimators\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "rf_grid_search_results[\"max_depth\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "rf_grid_search_results[\"min_samples_split\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_split\"])\n",
    "rf_grid_search_results[\"min_samples_leaf\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_leaf\"])\n",
    "rf_grid_search_results[\"max_features\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_features\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_grid_search_results = rf_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models\n",
    "rf_grid_search_results.sort_values(\"validation_rmse\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b61928-a213-4474-a934-17684a2d041e",
   "metadata": {},
   "source": [
    "### XGBoost Regressor  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:  \n",
    "    - `n_estimators=100`: Number of trees.  \n",
    "    - `max_depth=6`: Maximum depth of each tree.  \n",
    "    - `learning_rate=0.3`: Step size shrinking to prevent overfitting.  \n",
    "    - `subsample=1.0`: Fraction of training samples used for each tree.  \n",
    "    - `colsample_bytree=1.0`: Fraction of features used for each tree.  \n",
    "    - `colsample_bylevel=1.0`: Fraction of features used at each tree level.  \n",
    "    - `colsample_bynode=1.0`: Fraction of features used at each node.  \n",
    "- Regularization and Learning:  \n",
    "    - `gamma=0`: Minimum loss reduction required to make a further partition on a leaf node.  \n",
    "    - `min_child_weight=1`: Minimum sum of instance weight (hessian) in a child.  \n",
    "    - `scale_pos_weight=1`: Controls the balance of positive and negative weights; used for imbalanced datasets.  \n",
    "- Training Behavior:  \n",
    "    - `objective=\"reg:squarederror\"`: Objective function for regression; default is for squared error.  \n",
    "    - `booster=\"gbtree\"`: Booster type; options include `\"gbtree\"` (default), `\"gblinear\"`, and `\"dart\"`.  \n",
    "    - `tree_method=\"auto\"`: Tree construction algorithm; `\"auto\"` chooses based on system configuration. Options include `\"exact\"`, `\"approx\"`, and `\"hist\"`.  \n",
    "    - `eval_metric=\"rmse\"`: Metric used for validation during training; default is root mean square error (`rmse`).  \n",
    "- Performance Optimization:  \n",
    "    - `early_stopping_rounds=None`: Stops training if validation metric does not improve after specified rounds.  \n",
    "    - `n_jobs=1`: Number of threads used for parallel computation (`-1` for all processors).  \n",
    "    - `random_state=None`: Seed for reproducibility.  \n",
    "    - `verbose=1`: Verbosity level for training output; 0 for silent, higher values show more details.  \n",
    "- Advanced Parameters:  \n",
    "    - `lambda=1`: L2 regularization term on weights.  \n",
    "    - `alpha=0`: L1 regularization term on weights.  \n",
    "    - `max_delta_step=0`: Used to help with convergence in highly imbalanced datasets.\n",
    "\n",
    "For more details, refer to the official [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html).\n",
    "\n",
    "The following hyperparameters are typically the most impactful:\n",
    "- `n_estimators`\n",
    "- `max_depth` \n",
    "- `learning_rate`\n",
    "- `subsample` \n",
    "- `colsample_bytree`\n",
    "- `gamma`\n",
    "- `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f58f8-0c05-4a19-a851-ed03d87bc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],               \n",
    "    \"max_depth\": [3, 6, 10], \n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  \n",
    "    \"gamma\": [0, 0.1, 0.2],           \n",
    "    \"min_child_weight\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"neg_root_mean_squared_error\"  # use \"r2\" for R-squared or a custom function for MAPE\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "xgb_grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358dfa5-5366-495d-89fe-f76611299e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of grid search results \n",
    "xgb_grid_search_results = pd.DataFrame({\n",
    "    \"validation_rmse\": -1 * xgb_grid_search.cv_results_[\"mean_test_score\"],  # RMSE on validation data\n",
    "    \"parameters\": xgb_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each parameter as a separate column\n",
    "xgb_grid_search_results[\"n_estimators\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "xgb_grid_search_results[\"max_depth\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "xgb_grid_search_results[\"learning_rate\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"learning_rate\"])\n",
    "xgb_grid_search_results[\"subsample\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"subsample\"])\n",
    "xgb_grid_search_results[\"colsample_bytree\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"colsample_bytree\"])\n",
    "xgb_grid_search_results[\"gamma\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"gamma\"])\n",
    "xgb_grid_search_results[\"min_child_weight\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_child_weight\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_grid_search_results = xgb_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models\n",
    "xgb_grid_search_results.sort_values(\"validation_rmse\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef98c90-8aca-4cea-abae-91730047696c",
   "metadata": {},
   "source": [
    "## Final Model  \n",
    "Based on hyperparameter tuning, the following model achieved the best performance (e.g., RMSE = 12.34) on the validation data compared to other candidates, making it the optimal choice for the final model. This model will be further evaluated on the test data to confirm its generalizability.\n",
    "\n",
    "Example final model: XGBoost Regressor  \n",
    "Selected hyperparameters:\n",
    "- `n_estimators=300`\n",
    "- `max_depth=4` \n",
    "- `learning_rate=0.1`\n",
    "- `subsample=0.8` \n",
    "- `colsample_bytree=0.8`\n",
    "- `gamma=0.1`\n",
    "- `min_child_weight=3`\n",
    "- `random_state=42`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb70149-6fda-49df-a20b-10130e193888",
   "metadata": {},
   "source": [
    "### Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50760a6-e090-4481-8eea-223a0c5d1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "final_model = XGBRegressor(\n",
    "    n_estimators=300, \n",
    "    max_depth=4, \n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=0.8, \n",
    "    gamma=0.1,\n",
    "    min_child_weight=3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "final_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5419d06-3048-4ae6-a9a0-f92eea024c99",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d565f-4c82-430f-89e3-cf03f326696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training, validation and test data\n",
    "y_train_pred = final_model.predict(X_train_transformed)\n",
    "y_val_pred = final_model.predict(X_val_transformed)\n",
    "y_test_pred = final_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate model\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "val_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
    "val_mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Create table of evaluation metrics\n",
    "final_model_evaluation = pd.DataFrame({\n",
    "    \"Metric\": [\"RMSE\", \"MAPE\", \"R-squared\"],\n",
    "    \"Training\": [train_rmse, train_mape, train_r2],\n",
    "    \"Validation\": [val_rmse, val_mape, val_r2],\n",
    "    \"Test\": [test_rmse, test_mape, test_r2]\n",
    "})\n",
    "\n",
    "# Show evaluation metrics\n",
    "print(final_model_evaluation.round(2))  # round metrics to 2 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b15d30-b95b-4499-ac84-2124b938669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resiudal plots for validation data\n",
    "plot_residuals(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec2f02-1f58-4004-8a56-82e43e7e460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resiudal plots for tets data\n",
    "plot_residuals(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cd43e-206f-464e-a2f8-06f3c641a06f",
   "metadata": {},
   "source": [
    "### Feature Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c656ef-62d4-4836-b919-c85e8549c92d",
   "metadata": {},
   "source": [
    "XGBoost Feature Importance Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b47507-6281-4a28-961c-5dc8db8e01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of feature names for better readability\n",
    "feature_mapping = {\n",
    "    \"f0\": \"feature_1\",\n",
    "    \"f1\": \"feature_2\",\n",
    "    \"f2\": \"feature_3\",\n",
    "    \"f3\": \"feature_4\",\n",
    "    \"f4\": \"feature_5\",\n",
    "    \"f5\": \"feature_6\",\n",
    "    \"f6\": \"feature_7\",\n",
    "    \"f7\": \"feature_8\",\n",
    "    \"f8\": \"feature_9\",\n",
    "    \"f9\": \"feature_10\"\n",
    "}\n",
    "\n",
    "# Create feature importance plot of the top 10 features\n",
    "# Note: final_model must be an XGBoost model\n",
    "xgb.plot_importance(final_model, max_num_features=10)\n",
    "\n",
    "# Map the feature names\n",
    "plt.gca().set_yticklabels([feature_mapping.get(label.get_text(), label.get_text()) \n",
    "                           for label in plt.gca().get_yticklabels()])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661472f1-6c53-4270-a381-f96641708735",
   "metadata": {},
   "source": [
    "### Saving Model  \n",
    "Save both the column transformer and the final model as pickle files in the `models` directory for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bf3c1-3bc1-47a4-8c78-9ecf3052c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save column transformer \n",
    "try:\n",
    "    with open(\"models/column_transformer.pkl\", \"wb\") as file:\n",
    "        pickle.dump(column_transformer, file)\n",
    "    print(\"Column transformer saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the column transformer: {e}}\")\n",
    "    \n",
    "# Save final model\n",
    "try:\n",
    "    with open(\"models/final_model.pkl\", \"wb\") as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    print(\"Final model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the final model: {e}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482eca8-6853-44dc-ad83-7f4b4e5a31f8",
   "metadata": {},
   "source": [
    "# Modeling (Classification Task)  \n",
    "For a classification problem, where the goal is to predict a categorical target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da39af8-e1d1-4b70-b9e6-ba118f7aa2c6",
   "metadata": {},
   "source": [
    "## Training Baseline Models (Individually) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328333fe-aed7-45c2-b7f0-653eb0ddbc2e",
   "metadata": {},
   "source": [
    "### Logistic Regression  \n",
    "Hyperparameters and Default Values:\n",
    "- `penalty=\"l2\"`: Regularization type (`\"l1\"`, `\"l2\"`, `\"elasticnet\"`, or `\"none\"`).\n",
    "- `C=1.0`: Inverse of regularization strength (smaller = stronger regularization).\n",
    "- `solver=\"lbfgs\"`: Optimization algorithm (`\"lbfgs\"`, `\"liblinear\"`, `\"saga\"`, etc.).\n",
    "- `max_iter=100`: Maximum iterations for convergence.\n",
    "- `multi_class=\"auto\"`: Multi-class handling (`\"ovr\"` or `\"multinomial\"`).\n",
    "- `fit_intercept=True`: Whether to calculate the intercept.\n",
    "- `class_weight=None`: Class weights; `\"balanced\"` adjusts for imbalance.\n",
    "\n",
    "For more details, refer to the official [scikit-learn LogisticRegression documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeab2b1-5c84-43bb-8031-e3dc49c78cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train model\n",
    "lr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = lr.predict(X_train_transformed)\n",
    "y_val_pred = lr.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model: Classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate model: Confusion matrix \n",
    "print(\"Training Confusion Matrix:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=lr.classes_)\n",
    "cm_disp_train.plot(cmap=\"Blues\")\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()b\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=lr.classes_)\n",
    "cm_disp_val.plot(cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e6c57-8d32-44ae-9f04-7592a27bb197",
   "metadata": {},
   "source": [
    "### Support Vector Classifier  \n",
    "Hyperparameters and Default Values:\n",
    "- `C=1.0`: Regularization strength (smaller = stronger regularization).\n",
    "- `kernel=\"rbf\"`: Kernel type (`\"linear\"`, `\"poly\"`, `\"rbf\"`, `\"sigmoid\"`, or callable).\n",
    "- `degree=3`: Degree of the polynomial kernel (ignored for other kernels).\n",
    "- `gamma=\"scale\"`: Kernel coefficient (`\"scale\"`, `\"auto\"`, or float).\n",
    "- `coef0=0.0`: Independent term in kernel functions (`\"poly\"` and `\"sigmoid\"` only).\n",
    "- `class_weight=None`: Class weights; `\"balanced\"` adjusts for imbalance.\n",
    "- `max_iter=-1`: Maximum iterations (unlimited if `-1`).\n",
    "- `probability=False`: Enables probability estimates (slower training).\n",
    "- `random_state=None`: Seed for reproducibility (affects `shrinking` and `probability`).\n",
    "\n",
    "For more details, refer to the official [scikit-learn SVC documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176f017-440e-475e-ae4e-f3240d790887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "svc = SVC()  \n",
    "\n",
    "# Train model\n",
    "svc.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = svc.predict(X_train_transformed)\n",
    "y_val_pred = svc.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model: Classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate model: Confusion matrix\n",
    "print(\"Training Confusion Matrix:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=svc.classes_)\n",
    "cm_disp_train.plot(cmap=\"Blues\")\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=svc.classes_)\n",
    "cm_disp_val.plot(cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c6880-b130-4b52-991d-f434d00f3d5e",
   "metadata": {},
   "source": [
    "### Random Forest Classifier  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:\n",
    "  - `n_estimators=100`: Number of trees in the forest.\n",
    "  - `max_depth=None`: Maximum depth of each tree; `None` allows trees to grow until all leaves are pure or minimum samples are reached.\n",
    "  - `min_samples_split=2`: Minimum number of samples required to split a node.\n",
    "  - `min_samples_leaf=1`: Minimum number of samples required at a leaf node.\n",
    "  - `max_features=\"auto\"`: Number of features considered for the best split; default `auto` uses the square root of all features.\n",
    "- Regularization:\n",
    "  - `max_leaf_nodes=None`: Maximum number of leaf nodes per tree.\n",
    "  - `min_impurity_decrease=0.0`: Splits a node only if it decreases impurity by this threshold.\n",
    "  - `class_weight=None`: Weights associated with classes. If `None`, all classes are supposed to have weight one. Use `\"balanced\"` to automatically adjust weights inversely proportional to class frequencies in the input data.\n",
    "- Training Behavior:\n",
    "  - `bootstrap=True`: Whether to use bootstrap samples for training each tree.\n",
    "  - `oob_score=False`: Whether to use out-of-bag samples to estimate prediction accuracy.\n",
    "  - `n_jobs=None`: Number of CPU threads used (`-1` for all processors).\n",
    "  - `random_state=None`: Random seed for reproducibility.\n",
    "  - `verbose=0`: Controls the verbosity of output during training.\n",
    "- Performance Optimization:\n",
    "  - `max_samples=None`: Maximum number of samples used to train each tree, useful for subsampling large datasets.\n",
    "\n",
    "For more details, refer to the official [scikit-learn RandomForestClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037ba56-e990-409f-9c19-5d15ec0c68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train model\n",
    "rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = rf.predict(X_train_transformed)\n",
    "y_val_pred = rf.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model: Classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate model: Confusion matrix \n",
    "print(\"Training Confusion Matrix:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=rf.classes_)\n",
    "cm_disp_train.plot(cmap=\"Blues\")\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=rf.classes_)\n",
    "cm_disp_val.plot(cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c45832c-76dc-4826-a7a7-100534f785e2",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Classifier  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Architecture\n",
    "    - `hidden_layer_sizes=(100,)`: Number and size of hidden layers; e.g., `(100,)` = 1 layer with 100 neurons.\n",
    "    - `activation=\"relu\"`: Activation function; options: `\"relu\"`, `\"tanh\"`, `\"logistic\"`, `\"identity\"`.\n",
    "- Optimization\n",
    "    - `solver=\"adam\"`: Optimization algorithm; options: `\"adam\"`, `\"lbfgs\"`, `\"sgd\"`.\n",
    "    - `alpha=0.0001`: L2 regularization to reduce overfitting.\n",
    "    - `learning_rate=\"constant\"`: Learning rate strategy; options: `\"constant\"`, `\"invscaling\"`, `\"adaptive\"`.\n",
    "    - `learning_rate_init=0.001`: Initial learning rate.\n",
    "    - `power_t=0.5`: Used for `\"invscaling\"` learning rate.\n",
    "- Training Behavior\n",
    "    - `max_iter=200`: Maximum training iterations.\n",
    "    - `tol=1e-4`: Stopping criterion for improvement tolerance.\n",
    "    - `momentum=0.9`: Momentum for SGD (used with `solver=\"sgd\"`).\n",
    "    - `early_stopping=False`: Stop early if no improvement on validation set.\n",
    "    - `n_iter_no_change=10`: Iterations without improvement to stop training.\n",
    "- Performance\n",
    "    - `batch_size=\"auto\"`: Batch size; `\"auto\"` = `min(200, n_samples)`.\n",
    "    - `shuffle=True`: Shuffle training data every epoch.\n",
    "    - `random_state=None`: Seed for reproducibility.\n",
    "    - `verbose=False`: Control output verbosity.\n",
    "    - `warm_start=False`: Retain model state for further training.\n",
    "- Classification-Specific\n",
    "    - `validation_fraction=0.1`: Fraction of data for validation (used with `early_stopping=True`).\n",
    "    - `out_activation_=\"softmax\"`: Output activation for multi-class classification.\n",
    "\n",
    "For more details, refer to the official [scikit-learn MLPClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16330aa7-b3ef-4a0e-884b-0824ecf9c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "\n",
    "# Train model\n",
    "mlp.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = mlp.predict(X_train_transformed)\n",
    "y_val_pred = mlp.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model: Classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate model: Confusion matrix\n",
    "print(\"Training Confusion Matrix:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=mlp.classes_)\n",
    "cm_disp_train.plot(cmap=\"Blues\")\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=mlp.classes_)\n",
    "cm_disp_val.plot(cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801b4a3-2733-4b58-8962-8f7761fccf91",
   "metadata": {},
   "source": [
    "### XGBoost Classifier  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:  \n",
    "    - `n_estimators=100`: Number of trees (boosting rounds).  \n",
    "    - `max_depth=6`: Maximum depth of each tree.  \n",
    "    - `learning_rate=0.3`: Step size shrinkage to prevent overfitting.  \n",
    "    - `subsample=1.0`: Fraction of training samples used per tree.  \n",
    "    - `colsample_bytree=1.0`: Fraction of features used per tree.  \n",
    "    - `colsample_bylevel=1.0`: Fraction of features used per tree level.  \n",
    "    - `colsample_bynode=1.0`: Fraction of features used per split node.  \n",
    "- Regularization and Learning:  \n",
    "    - `gamma=0`: Minimum loss reduction required to split a leaf node.  \n",
    "    - `min_child_weight=1`: Minimum sum of instance weights (hessian) in a child.  \n",
    "    - `scale_pos_weight=1`: Balances positive and negative class weights for imbalanced datasets.  \n",
    "- Training Behavior:  \n",
    "    - `objective=\"binary:logistic\"`: Objective function for binary classification; alternatives include `\"multi:softmax\"` or `\"multi:softprob\"`.  \n",
    "    - `booster=\"gbtree\"`: Booster type; options include `\"gbtree\"` (default), `\"gblinear\"`, and `\"dart\"`.  \n",
    "    - `tree_method=\"auto\"`: Tree construction algorithm; options: `\"exact\"`, `\"approx\"`, `\"hist\"`, `\"gpu_hist\"`.  \n",
    "    - `eval_metric=\"logloss\"`: Default evaluation metric for binary classification. Options include `\"error\"`, `\"auc\"`, and others.  \n",
    "- Performance Optimization:  \n",
    "    - `early_stopping_rounds=None`: Stops training if validation metric does not improve after specified rounds.  \n",
    "    - `n_jobs=1`: Number of threads for parallel computation (`-1` for all processors).  \n",
    "    - `random_state=None`: Seed for reproducibility.  \n",
    "    - `verbose=1`: Verbosity level; 0 for silent, higher values for detailed output.  \n",
    "- Advanced Parameters:  \n",
    "    - `lambda=1`: L2 regularization term on weights.  \n",
    "    - `alpha=0`: L1 regularization term on weights.  \n",
    "    - `max_delta_step=0`: Helps with convergence in imbalanced datasets.\n",
    "\n",
    "For more details, refer to the official [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd78bba-62bf-447f-9dc7-6dda7e0dcdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Train model\n",
    "xgb.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predict on the training and validation data\n",
    "y_train_pred = xgb.predict(X_train_transformed)\n",
    "y_val_pred = xgb.predict(X_val_transformed)\n",
    "\n",
    "# Evaluate model: Classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate model: Confusion matrix \n",
    "print(\"Training Confusion Matrix:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=xgb.classes_)\n",
    "cm_disp_train.plot(cmap=\"Blues\")\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=xgb.classes_)\n",
    "cm_disp_val.plot(cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6983589-27f9-43d1-a8f3-ea43240fd89f",
   "metadata": {},
   "source": [
    "## Training Baseline Models (Pipeline)  \n",
    "Train five baseline models:\n",
    "- Logistic Regression\n",
    "- Support Vector Classifier\n",
    "- Random Forest Classifier\n",
    "- Multi-Layer Perceptron Classifier\n",
    "- XGBoost Classifier\n",
    "\n",
    "The model performance will be evaluated using the following metrics:  \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e58d6b-18f1-4344-a275-4ce620191bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with baseline configurations\n",
    "models = [LogisticRegression(), SVC(), RandomForestClassifier(random_state=42), MLPClassifier(random_state=42), XGBClassifier(random_state=42)]\n",
    "\n",
    "# Loop through each model\n",
    "for model in models:\n",
    "    # Show model\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Scale numerical columns and encode categorical columns \n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"scaler\", StandardScaler(), numerical_columns),\n",
    "            (\"encoder\", OneHotEncoder(drop=\"first\"), categorical_columns)\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Include the boolean columns without transformation\n",
    "    )\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"column_transformer\", column_transformer),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "    # Evaluate model: Classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    \n",
    "    # Evaluate model: Confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "    cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val)\n",
    "    cm_disp_val.plot(cmap=\"Blues\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccfe859-3f44-4ecb-b975-56e392b352ec",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning  \n",
    "Based on baseline model training, the following models outperformed the other candidates on evaluation metrics such as accuracy, recall, precision, and F1 score on the validation data and were selected for hyperparameter tuning:\n",
    "- Model 1 (e.g., Random Forest Classifier)  \n",
    "  *Justification example*: Demonstrated high recall and F1 score, indicating robust performance across multiple classes and strong handling of imbalanced datasets. \n",
    "- Model 2 (e.g., XGBoost Classifier)  \n",
    "  *Justification example*: Consistently high accuracy across classes and strong overall evaluation metric scores, making it a competitive option for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce66a1c-d2e8-4819-a35a-9b23545a0700",
   "metadata": {},
   "source": [
    "### Random Forest Classifier  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:\n",
    "  - `n_estimators=100`: Number of trees in the forest.\n",
    "  - `max_depth=None`: Maximum depth of each tree; `None` allows trees to grow until all leaves are pure or minimum samples are reached.\n",
    "  - `min_samples_split=2`: Minimum number of samples required to split a node.\n",
    "  - `min_samples_leaf=1`: Minimum number of samples required at a leaf node.\n",
    "  - `max_features=\"auto\"`: Number of features considered for the best split; default `auto` uses the square root of all features.\n",
    "- Regularization:\n",
    "  - `max_leaf_nodes=None`: Maximum number of leaf nodes per tree.\n",
    "  - `min_impurity_decrease=0.0`: Splits a node only if it decreases impurity by this threshold.\n",
    "  - `class_weight=None`: Weights associated with classes. If `None`, all classes are supposed to have weight one. Use `\"balanced\"` to automatically adjust weights inversely proportional to class frequencies in the input data.\n",
    "- Training Behavior:\n",
    "  - `bootstrap=True`: Whether to use bootstrap samples for training each tree.\n",
    "  - `oob_score=False`: Whether to use out-of-bag samples to estimate prediction accuracy.\n",
    "  - `n_jobs=None`: Number of CPU threads used (`-1` for all processors).\n",
    "  - `random_state=None`: Random seed for reproducibility.\n",
    "  - `verbose=0`: Controls the verbosity of output during training.\n",
    "- Performance Optimization:\n",
    "  - `max_samples=None`: Maximum number of samples used to train each tree, useful for subsampling large datasets.\n",
    "\n",
    "For more details, refer to the official [scikit-learn RandomForestClassifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier).\n",
    "\n",
    "The following hyperparameters are typically the most impactful:\n",
    "- `n_estimators`\n",
    "- `max_depth`  \n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `max_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae75e06-4ef6-469b-856e-e8f219f6b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "rf_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],               \n",
    "    \"max_depth\": [None, 10, 20],              \n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"max_features\": [0.33, 0.66, 1]                \n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf, \n",
    "    param_grid=rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"accuracy\"  # use \"f1\", \"precision\", \"recall\" or \"roc_auc\" optionally\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "rf_grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2528d8-f322-454c-b7dd-d100bfea0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of grid search results \n",
    "rf_grid_search_results = pd.DataFrame({\n",
    "    \"validation_accuracy\": rf_grid_search.cv_results_[\"mean_test_score\"],  # accuracy on validation data\n",
    "    \"parameters\": rf_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each parameter as a separate column\n",
    "rf_grid_search_results[\"n_estimators\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "rf_grid_search_results[\"max_depth\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "rf_grid_search_results[\"min_samples_split\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_split\"])\n",
    "rf_grid_search_results[\"min_samples_leaf\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_samples_leaf\"])\n",
    "rf_grid_search_results[\"max_features\"] = rf_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_features\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "rf_grid_search_results = rf_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models\n",
    "print(rf_grid_search_results.sort_values(\"validation_accuracy\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d7493-acb4-4043-891b-588621ff1872",
   "metadata": {},
   "source": [
    "### XGBoost Classifier  \n",
    "Hyperparameters and Default Values:\n",
    "- Model Complexity:  \n",
    "    - `n_estimators=100`: Number of trees (boosting rounds).  \n",
    "    - `max_depth=6`: Maximum depth of each tree.  \n",
    "    - `learning_rate=0.3`: Step size shrinkage to prevent overfitting.  \n",
    "    - `subsample=1.0`: Fraction of training samples used per tree.  \n",
    "    - `colsample_bytree=1.0`: Fraction of features used per tree.  \n",
    "    - `colsample_bylevel=1.0`: Fraction of features used per tree level.  \n",
    "    - `colsample_bynode=1.0`: Fraction of features used per split node.  \n",
    "- Regularization and Learning:  \n",
    "    - `gamma=0`: Minimum loss reduction required to split a leaf node.  \n",
    "    - `min_child_weight=1`: Minimum sum of instance weights (hessian) in a child.  \n",
    "    - `scale_pos_weight=1`: Balances positive and negative class weights for imbalanced datasets.  \n",
    "- Training Behavior:  \n",
    "    - `objective=\"binary:logistic\"`: Objective function for binary classification; alternatives include `\"multi:softmax\"` or `\"multi:softprob\"`.  \n",
    "    - `booster=\"gbtree\"`: Booster type; options include `\"gbtree\"` (default), `\"gblinear\"`, and `\"dart\"`.  \n",
    "    - `tree_method=\"auto\"`: Tree construction algorithm; options: `\"exact\"`, `\"approx\"`, `\"hist\"`, `\"gpu_hist\"`.  \n",
    "    - `eval_metric=\"logloss\"`: Default evaluation metric for binary classification. Options include `\"error\"`, `\"auc\"`, and others.  \n",
    "- Performance Optimization:  \n",
    "    - `early_stopping_rounds=None`: Stops training if validation metric does not improve after specified rounds.  \n",
    "    - `n_jobs=1`: Number of threads for parallel computation (`-1` for all processors).  \n",
    "    - `random_state=None`: Seed for reproducibility.  \n",
    "    - `verbose=1`: Verbosity level; 0 for silent, higher values for detailed output.  \n",
    "- Advanced Parameters:  \n",
    "    - `lambda=1`: L2 regularization term on weights.  \n",
    "    - `alpha=0`: L1 regularization term on weights.  \n",
    "    - `max_delta_step=0`: Helps with convergence in imbalanced datasets.\n",
    "\n",
    "For more details, refer to the official [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html).\n",
    "\n",
    "The following hyperparameters are typically the most impactful:\n",
    "- `n_estimators`\n",
    "- `max_depth` \n",
    "- `learning_rate`\n",
    "- `subsample` \n",
    "- `colsample_bytree`\n",
    "- `gamma`\n",
    "- `min_child_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466fb3cb-e92a-41ef-aa6d-5ca2bffe0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid \n",
    "xgb_param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300, 500],               \n",
    "    \"max_depth\": [3, 6, 10], \n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],  \n",
    "    \"gamma\": [0, 0.1, 0.2],           \n",
    "    \"min_child_weight\": [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize grid search object\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=xgb_param_grid, \n",
    "    cv=5, \n",
    "    scoring=\"accuracy\"  # use \"f1\", \"precision\", \"recall\" or \"roc_auc\" optionally\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "xgb_grid_search.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7e442-ad81-4970-8400-203268267790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of grid search results \n",
    "xgb_grid_search_results = pd.DataFrame({\n",
    "    \"validation_accuracy\": xgb_grid_search.cv_results_[\"mean_test_score\"],  # accuracy on validation data\n",
    "    \"parameters\": xgb_grid_search.cv_results_[\"params\"]  # parameter values\n",
    "}) \n",
    "\n",
    "# Extract each parameter as a separate column\n",
    "xgb_grid_search_results[\"n_estimators\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"n_estimators\"])\n",
    "xgb_grid_search_results[\"max_depth\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"max_depth\"])\n",
    "xgb_grid_search_results[\"learning_rate\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"learning_rate\"])\n",
    "xgb_grid_search_results[\"subsample\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"subsample\"])\n",
    "xgb_grid_search_results[\"colsample_bytree\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"colsample_bytree\"])\n",
    "xgb_grid_search_results[\"gamma\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"gamma\"])\n",
    "xgb_grid_search_results[\"min_child_weight\"] = xgb_grid_search_results[\"parameters\"].apply(lambda x: x[\"min_child_weight\"])\n",
    "\n",
    "# Delete the parameters column\n",
    "xgb_grid_search_results = xgb_grid_search_results.drop(\"parameters\", axis=1)\n",
    "\n",
    "# Show the top 10 best performing models\n",
    "print(xgb_grid_search_results.sort_values(\"validation_accuracy\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50d413-ace8-4005-b273-cae892faad54",
   "metadata": {},
   "source": [
    "## Final Model  \n",
    "Based on hyperparameter tuning, the following model achieved the best performance (e.g., accuracy = 0.92) on the validation data compared to other candidates, making it the optimal choice for the final model. This model will be further evaluated on the test data to confirm its generalizability.\n",
    "\n",
    "Example final model: XGBoost Classifier    \n",
    "Selected hyperparameters:\n",
    "- `n_estimators=300`\n",
    "- `max_depth=4` \n",
    "- `learning_rate=0.1`\n",
    "- `subsample=0.8` \n",
    "- `colsample_bytree=0.8`\n",
    "- `gamma=0.1`\n",
    "- `min_child_weight=3`\n",
    "- `random_state=42`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3ea9b-9d33-4bb8-851c-645471bb427d",
   "metadata": {},
   "source": [
    "### Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362036ce-5d52-4ba6-b0ac-bf7cd78742ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "final_model = XGBClassifier(\n",
    "    n_estimators=300, \n",
    "    max_depth=4, \n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8, \n",
    "    colsample_bytree=0.8, \n",
    "    gamma=0.1,\n",
    "    min_child_weight=3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "final_model.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4906c4-3902-4059-9613-e273e834f4b5",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f3ac3-d961-4a3d-89c4-f9186d6cada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training, validation and test data\n",
    "y_train_pred = final_model.predict(X_train_transformed)\n",
    "y_val_pred = final_model.predict(X_val_transformed)\n",
    "y_test_pred = final_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate model: Classification report\n",
    "print(\"Training Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Evaluate model: Confusion matrix\n",
    "print(\"Training Confusion Matrix:\")\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "cm_disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=final_model.classes_)\n",
    "cm_disp_train.plot(cmap=\"Blues\")\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "cm_disp_val = ConfusionMatrixDisplay(confusion_matrix=cm_val, display_labels=final_model.classes_)\n",
    "cm_disp_val.plot(cmap=\"Blues\")\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Test Confusion Matrix:\")\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "cm_disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=final_model.classes_)\n",
    "cm_disp_test.plot(cmap=\"Blues\")\n",
    "plt.title(\"Test Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e63402e-85d3-445c-be3c-074f350a97fc",
   "metadata": {},
   "source": [
    "### Feature Importance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65833257-d9e7-4162-99aa-135fadddf6d8",
   "metadata": {},
   "source": [
    "XGBoost Feature Importance Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd242e0-bd0a-422e-ac3b-4ccda65901a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of feature names for better readability\n",
    "feature_mapping = {\n",
    "    \"f0\": \"feature_1\",\n",
    "    \"f1\": \"feature_2\",\n",
    "    \"f2\": \"feature_3\",\n",
    "    \"f3\": \"feature_4\",\n",
    "    \"f4\": \"feature_5\",\n",
    "    \"f5\": \"feature_6\",\n",
    "    \"f6\": \"feature_7\",\n",
    "    \"f7\": \"feature_8\",\n",
    "    \"f8\": \"feature_9\",\n",
    "    \"f9\": \"feature_10\"\n",
    "}\n",
    "\n",
    "# Create feature importance plot of the top 10 features\n",
    "# Note: final_model must be an XGBoost model\n",
    "xgb.plot_importance(final_model, max_num_features=10)\n",
    "\n",
    "# Map the feature names\n",
    "plt.gca().set_yticklabels([feature_mapping.get(label.get_text(), label.get_text()) \n",
    "                           for label in plt.gca().get_yticklabels()])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc2ed5-4d97-478f-ac1b-725a6de4fbd1",
   "metadata": {},
   "source": [
    "### Saving Model  \n",
    "Save both the column transformer and the final model as pickle files in the `models` directory for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee620100-347a-48f9-a1d7-9228f1556572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save column transformer \n",
    "try:\n",
    "    with open(\"models/column_transformer.pkl\", \"wb\") as file:\n",
    "        pickle.dump(column_transformer, file)\n",
    "    print(\"Column transformer saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the column transformer: {e}}\")\n",
    "    \n",
    "# Save final model\n",
    "try:\n",
    "    with open(\"models/final_model.pkl\", \"wb\") as file:\n",
    "        pickle.dump(final_model, file)\n",
    "    print(\"Final model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving the final model: {e}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8612fd-291f-4038-a180-35ed168eaca5",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ae0de-8df6-4a3a-8c2e-404e167fe1ab",
   "metadata": {},
   "source": [
    "**Data Preprocessing**:\n",
    "- Removed duplicates (e.g., based on the ID column) using `drop_duplicates` from `pandas`.\n",
    "- Handled incorrect data types (e.g., converted string columns to numerical or datetime columns) using `astype` or `to_datetime` from `pandas`.\n",
    "- Extracted features (e.g., created categorical, numerical, or boolean features from string columns) using custom functions with `apply` from `pandas`, `lambda` functions, and `re` for pattern matching.\n",
    "- Handled missing values (e.g., through deletion, median imputation for numerical columns, or mode imputation for categorical columns) using `dropna` or `fillna` from `pandas`.\n",
    "- Handled outliers (e.g., removed them using the 3SD method or 1.5 IQR method) with a custom transformer class utilizing `BaseEstimator` and `TransformerMixin` from `sklearn`.\n",
    "- Saved the preprocessed data as a .csv file using `to_csv` from `pandas` or in a MySQL database table using `sqlalchemy`, `mysql-connector-python`, and `to_sql` from `pandas`.\n",
    "- Split data into training (e.g., 70%), validation (15%), and test (15%) sets using `train_test_split` from `sklearn`.\n",
    "- Scaled numerical features (e.g., using standard scaling or min-max normalization) with `StandardScaler` or `MinMaxScaler` from `sklearn`.\n",
    "- Encoded categorical features (e.g., using one-hot encoding) with `OneHotEncoder` from `sklearn`.\n",
    "\n",
    "**Exploratory Data Analysis (EDA)**:\n",
    "- Analyzed descriptive statistics (e.g., mean, median, standard deviation) of numerical columns and visualized distributions (e.g., using histograms).\n",
    "- Examined frequencies of categorical columns and visualized them (e.g., using bar plots or a bar plot matrix).\n",
    "- Assessed bivariate relationships between numerical columns (e.g., using a correlation heatmap, scatterplots, or a scatterplot matrix).\n",
    "- Explored relationships between numerical and categorical columns with group-wise statistics (e.g., mean or median by category) and visualized them (e.g., through bar plots or a bar plot matrix).\n",
    "\n",
    "**Modeling**:\n",
    "- Trained baseline models to establish performance benchmarks:\n",
    "    - For a regression task (e.g., Linear Regression, Support Vector Regressor, Random Forest Regressor, Multi-Layer Perceptron Regressor, XGBoost Regressor).\n",
    "    - For a classification task (e.g., Logistic Regression, Support Vector Classifier, Random Forest Classifier, Multi-Layer Perceptron Classifier, XGBoost Classifier).\n",
    "- Performed hyperparameter tuning (e.g., using grid search).\n",
    "- Selected the final model based on performance evaluation:\n",
    "    - For a regression task (e.g., using RMSE, MAPE, or R-squared)\n",
    "    - For a classification task (e.g., using accuracy, precision, recall, or F1 score).\n",
    "- Saved the final model (e.g., as a .pkl file using `pickle`).\n",
    "\n",
    "**Next Steps**:\n",
    "- Add K-Nearest Neighbors (KNN) as individual baseline model and in model pipeline for both regression and classification task. \n",
    "- Add Decision Tree as individual baseline model and in model pipeline for both regression and classification task.\n",
    "- Add Lasso, Ridge, and Polynomial Regression for modeling.\n",
    "- Add RandomSearchCV for hyperparameter tuning.\n",
    "- Add unsupervised learning section with algorithms like K-Means Clustering, DBSCAN, and Principal Component Analysis (PCA).\n",
    "- Add model deployment as a web application with an interactive UI and API (e.g., using Gradio or Flask)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-venv",
   "language": "python",
   "name": "machine-learning-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
