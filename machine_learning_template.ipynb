{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf7c4ae-408e-4aeb-9fe9-e20263d2ac7f",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f512f9-2618-4b1a-a361-69b63dcfdcd7",
   "metadata": {},
   "source": [
    "**Overview:** Brief description of the problem, the dataset, and the main objectives of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9288e0-3815-4c47-94c3-33d0a6aafc8e",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00680c12-3986-4262-9d6d-e330b55fd079",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100528be-33fb-4f95-b8f8-e791de38fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4896aa22-da27-4d06-b34f-d63ceace4ff0",
   "metadata": {},
   "source": [
    "## Environment Variables \n",
    "**Note**: Setting environment variables is optional, but it is recommended if you store sensitive information (such as API keys or database credentials) in a `.env` file. Using environment variables helps keep such information secure and separate from your codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bb3ec-8e57-460d-8b31-0508025f19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from .env \n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Get SQL database credentials from .env\n",
    "sql_username = os.getenv(\"SQL_USERNAME\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93f5c7-8968-4a0e-907f-a6bc60d0dd3d",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de96aef-7438-41fd-bd1d-8cdbdc0ed9cb",
   "metadata": {},
   "source": [
    "## csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d0cf0-e26b-4ab4-9070-cf96d67e5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a csv file into a Pandas DataFrame\n",
    "df = pd.read_csv(\"your_csv_file_here.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63482d3-000f-4235-a16a-51d6d8a08dba",
   "metadata": {},
   "source": [
    "## MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81da48-0441-4908-ba1a-990449b65ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database info\n",
    "mysql_host = \"localhost\"  # Default hostname for a MySQL server running locally\n",
    "mysql_port = 3306  # Default port for MySQL\n",
    "mysql_database_name = \"your_mysql_database_name_here\"\n",
    "mysql_table_name = \"your_mysql_table_name_here\"\n",
    "\n",
    "# Create an SQLAlchemy engine for interacting with the MySQL database\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{sql_username}:{sql_password}@{mysql_host}:{mysql_port}/{mysql_database_name}\")\n",
    "\n",
    "# Load data from MySQL database into a Pandas DataFrame\n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql(f\"SELECT * FROM {mysql_table_name}\", con=connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7809f-dd55-4350-a52d-7c20c761a102",
   "metadata": {},
   "source": [
    "# Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d22f7-282d-47ee-bfcf-e85352c450c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00719b-3f66-405d-97a0-79907e91ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad1422-e239-4d5d-b6e4-31661d8050be",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f1a0d-a98f-4446-8d44-d7e553e6230f",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee509a36-86a0-4e56-bbeb-ecb29a809288",
   "metadata": {},
   "source": [
    "Duplicates based on all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71162a8d-4f29-470c-a919-8494ef16e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose duplicates \n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0253293-b7c9-4335-9eef-61b29a09ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f43e8-f544-414c-8979-d3a52b0b919d",
   "metadata": {},
   "source": [
    "Duplicates based on specific columns, e.g. the ID column or a combination of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec11f9-42d4-4bd2-a769-c6affd2778d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose duplicates\n",
    "df.duplicated(subset=[\"column_1\", \"column_2\", \"column_3\"]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620ab52-9ff0-4b3d-a0d6-ee856fdfb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=[\"column_1\", \"column_2\", \"column_3\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28c409-34a3-493f-8cd9-856b5c25e0d3",
   "metadata": {},
   "source": [
    "## Handling Incorrect Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92d718-ec5a-491c-873b-f2b8230f71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from str to int\n",
    "df[\"int_column\"] = df[\"str_column\"].astype(\"Int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb41a-a715-4c64-a6ca-9db164c4dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column from str to datetime\n",
    "df[\"datetime_column\"] = pd.to_datetime(df[\"str_column\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff6c41-2244-462e-968a-9e8473a89ed6",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9931c-d4f6-4255-a894-2be60a02c26e",
   "metadata": {},
   "source": [
    "### Categorical Feature from String Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639472a6-9633-4972-b19f-e57b10ad2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a category from a string   \n",
    "def extract_category_from_string(string):\n",
    "    # Map categories to their corresponding list of keywords\n",
    "    category_keywords_map = {\n",
    "        \"Category 1\": [\"Keyword 1\", \"Keyword 2\", \"Keyword 3\"],\n",
    "        \"Category 2\": [\"Keyword 4\", \"Keyword 5\", \"Keyword 6\"],\n",
    "        \"Category 3\": [\"Keyword 7\", \"Keyword 8\", \"Keyword 9\"]\n",
    "    }\n",
    "\n",
    "    # Loop through each category and its associated keywords \n",
    "    for category, keywords in category_keywords_map.items():\n",
    "        # Check if any keyword is present in the string\n",
    "        if any(keyword in string for keyword in keywords):\n",
    "            return category  # Return the category corresponding to the keyword\n",
    "    return np.nan  # Return a missing value if no keyword matches\n",
    "\n",
    "# Apply function on an existing string column to create a new categorical feature column\n",
    "df[\"categorical_feature\"] = df[\"str_column\"].apply(extract_category_from_string)\n",
    "\n",
    "# Show category frequencies\n",
    "print(df[\"categorical_feature\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9089b-a2fd-4305-aa23-086332d45c01",
   "metadata": {},
   "source": [
    "### Numerical Feature from String Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41134f56-1d30-435d-9f32-6ff428d940a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "# Function to extract the first number in a string \n",
    "def extract_number_from_string(string):\n",
    "    first_number = re.search(r\"\\b-?\\d+([.,]\\d+)?\\b\", string)  # searches for first integer or float (positive or negative; decimal separator \".\" or \",\")\n",
    "    if first_number:\n",
    "        return float(first_number.group().replace(\",\", \".\"))  # Replace \",\" with \".\" as decimal separator  \n",
    "    else:\n",
    "        return np.nan  # Return a missing value if no number in string\n",
    "\n",
    "# Apply function on an existing string column to create a new numerical feature column\n",
    "df[\"numerical_feature\"] = df[\"str_column\"].apply(extract_number_from_string)\n",
    "\n",
    "# Show descriptive statistics of numerical feature\n",
    "df[\"numerical_feature\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba6942-83b2-4bd4-9223-94bfc963beef",
   "metadata": {},
   "source": [
    "### Boolean Feature from String Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5277ca-3ef2-4e5f-8b06-e2701b601f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords to determine if the feature is present or absent\n",
    "keywords = [\"keyword 1\", \"keyword 2\", \"keyword 3\"]\n",
    "\n",
    "# Extract boolean feature column: True if any keyword is found in the string column\n",
    "df[\"boolean_feature\"] = df[\"str_column\"].apply(lambda x: any(keyword.lower() in x.lower() for keyword in keywords))\n",
    "\n",
    "# Show frequencies of boolean feature\n",
    "print(df[\"boolean_feature\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4d1a9-db9b-4e28-ba65-986831dd8b38",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7806d-1465-4747-8780-b2e28d51a468",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c06ef-bbbf-4d18-bde4-e7a50fc1a1a3",
   "metadata": {},
   "source": [
    "Imputation for numerical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680d14e-5853-4c3f-ac50-d639b7033fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical column\n",
    "df[\"numerical_column\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ca973-3ef1-43a3-b690-69c0ed2f0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the median\n",
    "median = df[\"numerical_column\"].median()\n",
    "df[\"numerical_column\"] = df[\"numerical_column\"].fillna(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018d4b9-c440-4b15-aafb-a4bb3a3e79fa",
   "metadata": {},
   "source": [
    "Imputation for categorical column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638cab2-bc30-4ae3-9cc1-310b50d5cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequencies of categorical column\n",
    "df[\"categorical_column\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36266f3-6460-48cf-83af-0c888432655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with the mode \n",
    "mode = df[\"categorical_column\"].mode()[0]\n",
    "df[\"categorical_column\"] = df[\"categorical_column\"].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9519b6-a492-4308-abc0-89a470543652",
   "metadata": {},
   "source": [
    "### Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc408e-1e71-4ec1-a678-9b14e16878ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where any column has a missing value \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf359f7-2519-47cf-bb29-60d64918626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where either column_1 or column_2 has a missing value \n",
    "df.dropna(subset=[\"column_1\", \"column_2\"], how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e082e3-b3ab-4c7b-932b-339610614132",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5625577-0d32-402f-bac1-1d4184d765c6",
   "metadata": {},
   "source": [
    "### Remove with 3SD Method  \n",
    "Remove univariate outliers from a numerical column by applying the 3 standard deviation (SD) rule. Specifically, a data point is considered an outlier if it falls more than 3 standard deviations above or below the mean of the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2815589-353e-48a4-8941-4a56b037e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to remove outliers using the 3SD method\n",
    "class OutlierRemover3SD(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_column):\n",
    "        # Calculate mean, standard deviation, and cutoff values of the numerical column\n",
    "        self.mean_ = df[numerical_column].mean()\n",
    "        self.sd_ = df[numerical_column].std()\n",
    "        self.lower_cutoff_ = self.mean_ - 3 * self.sd_\n",
    "        self.upper_cutoff_ = self.mean_ + 3 * self.sd_\n",
    "\n",
    "        # Create a mask for filtering outliers\n",
    "        self.mask_ = (df[numerical_column] >= self.lower_cutoff_) & (df[numerical_column] <= self.upper_cutoff_)\n",
    "\n",
    "        # Show cutoff values and number of outliers\n",
    "        print(f\"Lower cutoff for {numerical_column}: {self.lower_cutoff_}\")\n",
    "        print(f\"Upper cutoff for {numerical_column}: {self.upper_cutoff_}\")\n",
    "        print(f\"{df.shape[0] - df[self.mask_].shape[0]} outliers removed based on {numerical_column} using the 3SD method.\")\n",
    "  \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the mask \n",
    "        return df[self.mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_column):\n",
    "        # Perform both fit and transform \n",
    "        return self.fit(df, numerical_column).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d2263-e9de-4b53-8ada-61e1cf8433b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_3sd = OutlierRemover3SD()\n",
    "\n",
    "# Remove outliers\n",
    "df = outlier_remover_3sd.fit_transform(df, \"numerical_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a030d86-a4da-4b67-bd83-11dcc2af33ec",
   "metadata": {},
   "source": [
    "### Remove with 1.5 IQR Method  \n",
    "Remove univariate outliers from a numerical column using the 1.5 interquartile range (IQR) rule. Specifically, a data point is considered an outlier if it falls more than 1.5 interquartile ranges above the third quartile (Q3) or below the first quartile (Q1) of the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83870620-24da-4801-9520-ed1a3650a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom transformer class to remove outliers using the 1.5 IQR method\n",
    "class OutlierRemoverIQR(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, df, numerical_column):\n",
    "        # Calculate quartiles, IQR and cutoff values \n",
    "        Q1 = df[numerical_column].quantile(0.25)\n",
    "        Q3 = df[numerical_column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        self.lower_cutoff_ = Q1 - 1.5 * IQR\n",
    "        self.upper_cutoff_ = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create a mask for filtering outliers\n",
    "        self.mask_ = (df[numerical_column] >= self.lower_cutoff_) & (df[numerical_column] <= self.upper_cutoff_)\n",
    "  \n",
    "        # Show cutoff values and number of outliers\n",
    "        print(f\"Lower cutoff for {numerical_column}: {self.lower_cutoff_}\")\n",
    "        print(f\"Upper cutoff for {numerical_column}: {self.upper_cutoff_}\")\n",
    "        print(f\"{df.shape[0] - df[self.mask_].shape[0]} outliers removed based on {numerical_column} using the 1.5 IQR method.\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        # Remove outliers based on the mask \n",
    "        return df[self.mask_]\n",
    "\n",
    "    def fit_transform(self, df, numerical_column):\n",
    "        # Perform both fit and transform\n",
    "        return self.fit(df, numerical_column).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798816c-e967-4cd5-a68d-a1bbe2028057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize outlier remover \n",
    "outlier_remover_iqr = OutlierRemoverIQR()\n",
    "\n",
    "# Remove outliers\n",
    "df = outlier_remover_iqr.fit_transform(df, \"numerical_column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca10b19-02a9-4a0f-b543-7374f677d139",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571beab2-bbb5-4fba-8143-69b04a5ea2c8",
   "metadata": {},
   "source": [
    "## Defining Column Types  \n",
    "Define numerical, categorical and boolean columns for downstream tasks like exploratory data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d48ac1-c330-43f6-b460-bf5b57c9acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns and their pandas data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f5252-3567-42a7-8ef2-13f52c6a4ec6",
   "metadata": {},
   "source": [
    "**Option 1: Manually**  \n",
    "Use this approach for small datasets or when you have specific requirements and need explicit control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6838b9e-f03b-4791-abd6-ab1c0565e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column types manually\n",
    "numerical_columns = [\"numerical_column_1\", \"numerical_column_2\", \"numerical_column_3\"]\n",
    "categorical_columns = [\"categorical_column_1\", \"categorical_column_2\", \"categorical_column_3\"]\n",
    "boolean_columns = [\"boolean_column_1\", \"boolean_column_2\", \"boolean_column_3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c7d33-ac8c-4385-8d16-8bdab200eeed",
   "metadata": {},
   "source": [
    "**Option 2: Programmatically**  \n",
    "Use this approach for large datasets or when you want to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce92eb-c2b8-4fcf-83c6-1ae149b0a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column types programmatically based on pandas data types\n",
    "numerical_columns = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "boolean_columns = df.select_dtypes(include=[\"bool\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe81ae6-559e-432b-8cf2-8643b13ef6dd",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ce7fc-350e-4624-949a-60d1386bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with descriptive statistics of all numerical columns\n",
    "df[numerical_columns].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895c96c-daff-4873-852d-615627e17dcb",
   "metadata": {},
   "source": [
    "## Numerical Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76c43e-ff1c-4477-8610-a103373b9383",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcf592-a21f-4155-8d27-b16cffd8cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of numerical target variable\n",
    "sns.histplot(df[\"numerical_target\"])\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"Distribution of numerical_target\")\n",
    "plt.xlabel(\"numerical_target\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6382adcc-1fdb-48b7-8f2a-6d27a559bbfc",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675dca5-3693-4669-8da3-6d97648bc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations between the numerical target variable and each numerical and boolean column\n",
    "combined_columns = numerical_columns + boolean_columns\n",
    "df[combined_columns].corr()[\"numerical_target\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c0cc0-34f1-4f4c-b8d0-2243ce6f4e4d",
   "metadata": {},
   "source": [
    "### Scatterplots \n",
    "Scatterplot matrix that shows scatterplots between the numerical target variable and each numerical column.   \n",
    "Example code for 9 scatterplots in a 3x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4a478-acd0-4f18-99f8-81f9247250bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size \n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# List of numerical columns excluding the target column\n",
    "numerical_columns_without_target = [col for col in numerical_columns if col != \"numerical_target\"]\n",
    "\n",
    "# Iterate over the numerical columns\n",
    "for i, numerical_column in enumerate(numerical_columns_without_target):\n",
    "    # Create a subplot in a 3x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    \n",
    "    # Create a scatterplot between the current column and the numerical target\n",
    "    sns.scatterplot(data=df, x=numerical_column, y=\"numerical_target\")\n",
    "    \n",
    "    # Add title and axis labels \n",
    "    plt.title(f\"numerical_target by {numerical_column}\")\n",
    "    plt.xlabel(f\"{numerical_column}\")\n",
    "    plt.ylabel(\"numerical_target\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdbe81-6e4d-48b4-ae69-07336924ce94",
   "metadata": {},
   "source": [
    "### Numerical Target Variable by Category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c58bee-b56d-4f5c-9b3f-ccf5cd47203c",
   "metadata": {},
   "source": [
    "Descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5d314-45ef-4eee-9cfa-918a52081ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics of numerical target variable by categorical column\n",
    "target_by_category = df[\"numerical_target\"].groupby(df[\"categorical_column\"])\n",
    "target_by_category.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0cd54-3004-408b-a1b6-4874e034fd71",
   "metadata": {},
   "source": [
    "Bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6c9f6-896f-4490-a125-3fe8ce91cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the median of the numerical target by category\n",
    "median_target_by_category = target_by_category.median()\n",
    "\n",
    "# Bar plot of median by category\n",
    "sns.barplot(x=median_target_by_category.index, y=median_target_by_category.values, palette=\"colorblind\")\n",
    "\n",
    "# Add title and axes labels\n",
    "plt.title(\"Median numerical_target by categorical_column\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"numerical_target\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec66e0-2f7f-49c4-847b-f7b696217640",
   "metadata": {},
   "source": [
    "Bar plot matrix that shows bar plots between the numerical target variable and each categorical column.  \n",
    "Example code for 5 bar plots in a 2x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6f05a-e6d8-4f98-9046-f2ad3cd56b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size to 12x6 inches\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for i, categorical_column in enumerate(categorical_columns):\n",
    "    # Create a subplot in a 2x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Create a bar plot of median numerical_target by the current categorical column\n",
    "    ax = sns.barplot(data=df, x=categorical_column, y=\"numerical_target\", estimator=np.median, ci=None)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(f\"Median numerical_target by {categorical_column}\")\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"numerical_target\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f9891-a8ab-4a10-9351-2be2e20a1e8e",
   "metadata": {},
   "source": [
    "## Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bdb25-d466-48d1-bf2f-f2168e7bd774",
   "metadata": {},
   "source": [
    "### Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735f52c-eb16-4658-a566-0fcebf883180",
   "metadata": {},
   "source": [
    "#### Single Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9fe5a-ecf0-4388-8a9d-88590fa48f89",
   "metadata": {},
   "source": [
    "Absolute and relative frequencies of a single categorical colum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7a462-d369-4340-97e6-51480f28e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute and relative frequencies \n",
    "absolute_frequencies = df[\"categorical_column\"].value_counts()\n",
    "relative_frequencies = absolute_frequencies / absolute_frequencies.sum() * 100\n",
    "\n",
    "# Show frequencies\n",
    "print(f\"Absolute frequencies:\\n {absolute_frequencies}\\n\")\n",
    "print(f\"Relative frequencies:\\n {relative_frequencies.round(1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057fa22-fe17-4ea5-8558-cd8454ac77d6",
   "metadata": {},
   "source": [
    "Bar plot of frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a30df-8cbe-42b6-925d-57d808f9099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot\n",
    "sns.barplot(x=absolute_frequencies.index, y=absolute_frequencies.values, palette=\"colorblind\")\n",
    "\n",
    "# Add title and axes labels \n",
    "plt.title(\"categorical_column\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Rotate x-axis tick labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44f3be-22bb-498c-9560-8a53cbe51cee",
   "metadata": {},
   "source": [
    "#### Multiple Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9fb2d-babe-4c40-a375-d697bc98ee5f",
   "metadata": {},
   "source": [
    "Absolute and relative frequencies of multiple categorical colums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c4b6e5-237c-47df-8d5d-862bd031ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store frequencies for multiple categorical columns\n",
    "absolute_frequencies_dict = {}\n",
    "relative_frequencies_dict = {}\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for categorical_column in categorical_columns:\n",
    "    # Calculate absolute and relative frequencies \n",
    "    absolute_frequencies = df[categorical_column].value_counts()\n",
    "    relative_frequencies = absolute_frequencies / absolute_frequencies.sum() * 100\n",
    "\n",
    "    # Store frequencies\n",
    "    absolute_frequencies_dict[categorical_column] = absolute_frequencies\n",
    "    relative_frequencies_dict[categorical_column] = relative_frequencies\n",
    "\n",
    "    # Show frequencies\n",
    "    print(categorical_column.upper())\n",
    "    print(f\"Absolute frequencies:\\n {absolute_frequencies}\\n\")\n",
    "    print(f\"Relative frequencies (%):\\n {relative_frequencies.round(1)}\")\n",
    "    print(\"=\" * 50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3edff6-44e4-4e07-abd7-38d1fcb178cf",
   "metadata": {},
   "source": [
    "Bar plot matrix that shows frequencies for multiple categorical columns.  \n",
    "Example code for 5 bar plots in a 2x3 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651f74b5-524c-4517-97bb-af5d7a401c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over the categorical columns\n",
    "for i, categorical_column in enumerate(categorical_columns):\n",
    "    # Create a subplot in a 2x3 grid (current subplot i+1 because subplot indices start at 1)\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Calculate frequencies for the current column\n",
    "    absolute_frequencies = df[categorical_column].value_counts()\n",
    "    \n",
    "    # Create bar plot for the current column\n",
    "    sns.barplot(x=absolute_frequencies.index, y=absolute_frequencies.values, estimator=np.median, ci=None)\n",
    "    \n",
    "    # Add title and axes labels\n",
    "    plt.title(categorical_column.title())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Rotate x-axis tick labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bed553-89be-4154-9775-d784adfc100f",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedb34a-4995-4e6d-9024-b526c26002a8",
   "metadata": {},
   "source": [
    "Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a94e3f-694d-4b05-9436-fbeaf82f748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numerical and boolean columns for correlation analysis\n",
    "combined_columns = numerical_columns + boolean_columns\n",
    "\n",
    "# Calculate the correlation matrix \n",
    "correlation_matrix = df[combined_columns].corr()\n",
    "\n",
    "# Round correlations to 2 decimals\n",
    "correlation_matrix = round(correlation_matrix, 2)\n",
    "\n",
    "# Create an upper triangle mask (k=1 excludes the diagonal)\n",
    "mask = np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "\n",
    "# Set upper triangle to NaN to avoid redundant information\n",
    "correlation_matrix[mask] = np.nan\n",
    "\n",
    "# Show correlation matrix\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83902fe5-8ac2-4f64-bf9b-33e84fefb781",
   "metadata": {},
   "source": [
    "Correlation heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6eda3-d6aa-4578-8f55-d80d3e7153aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    cmap=\"viridis\",  # Color map choice\n",
    "    annot=True,  # Show numbers\n",
    "    linewidth=0.5  # Thin white lines between cells\n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92936fd-c3a7-4441-83b1-7bca48db2323",
   "metadata": {},
   "source": [
    "Save correlation heatmap as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8b750-799b-4b7b-8ba4-7cb2cafc9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "# Create \"images\" directory if it doesn't exist\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Save the heatmap as a .png image\n",
    "try:\n",
    "    # Construct full file path\n",
    "    image_path = os.path.join(\"images\", \"correlation_heatmap.png\") \n",
    "    \n",
    "    # Save the heatmap  \n",
    "    plt.savefig(\n",
    "        image_path, \n",
    "        bbox_inches=\"tight\",  # removes unnecessary whitespace\n",
    "        dpi=300  # higher image quality\n",
    "    )\n",
    "    print(f\"Correlation heatmap saved successfully to {image_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error saving correlation heatmap: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4d486-4d2c-4214-a260-ed19c446c41b",
   "metadata": {},
   "source": [
    "# Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c905a81-4c0d-4acf-85cd-04f81059d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X features and y target\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7fc1d-be3a-4623-9e16-aac8e85d3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and temporary sets (70% train, 30% temporary)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary data into validation and test sets (50% each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Delete the temporary data to free up memory\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88ff08-9c6f-44a6-ba05-74a1bbe9f0e6",
   "metadata": {},
   "source": [
    "Note: This accomplishes a 70% training, 15% validation and 15% test set size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b7d67-27ed-4716-8c3f-079e2d239339",
   "metadata": {},
   "source": [
    "# Regression Task  \n",
    "For a regression problem, where the goal is to predict a numerical target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21e2d4-cbd4-42f7-b7d1-f3997b3619f1",
   "metadata": {},
   "source": [
    "## Training Baseline Models  \n",
    "Train five baseline models:\n",
    "- Linear Regression\n",
    "- Support Vector Regressor\n",
    "- Random Forest Regressor\n",
    "- Multi-Layer Perceptron Regressor\n",
    "- XGBoost Regressor\n",
    "\n",
    "The model performance will be evaluated using the following metrics:  \n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- R-squared (R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a506f-0b4a-4aad-bfac-3e68f65f3dea",
   "metadata": {},
   "source": [
    "### Helper Function for Residual Plots  \n",
    "Creates two plots for predicted vs. actual target and residuals vs. actual target to evaluate model performance and identify potential issues in the model assumptions graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb007a-2d89-431e-ab84-60337886ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create residual plots\n",
    "def plot_residuals(y, y_pred):\n",
    "    # Calculate residuals\n",
    "    residuals = [actual_value - predicted_value for actual_value, predicted_value in zip(y, y_pred)]\n",
    "\n",
    "    # Create a 1x2 grid of subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), dpi=150)\n",
    "\n",
    "    # Plot 1: Predicted vs. Actual Target\n",
    "    axes[0].scatter(y, y_pred)\n",
    "    axes[0].plot([min(y), max(y)], \n",
    "                 [min(y), max(y)], \n",
    "                 color=\"red\", \n",
    "                 linestyle=\"--\", \n",
    "                 label=\"Perfect Prediction\")  # Add diagonal reference line\n",
    "    axes[0].set_title(\"Predicted vs. Actual Target\")\n",
    "    axes[0].set_xlabel(\"Actual Target\")\n",
    "    axes[0].set_ylabel(\"Predicted Target\")\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend() \n",
    "\n",
    "    # Plot 2: Residuals vs. Actual Target\n",
    "    axes[1].scatter(y, residuals)\n",
    "    axes[1].axhline(y=0, color=\"red\", linestyle=\"--\", label=\"Perfect Prediction\")  # Add horizontal reference line\n",
    "    axes[1].set_xlabel(\"Actual Target\")\n",
    "    axes[1].set_ylabel(\"Residuals\")\n",
    "    axes[1].set_title(\"Residuals vs. Actual Target\")\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend() \n",
    "\n",
    "    # Adjust layout and display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36172487-5eb6-495b-910e-aafdf0365a13",
   "metadata": {},
   "source": [
    "### Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca118a-6a69-4b27-adbc-4bdc77027ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models with baseline configurations\n",
    "models = [LinearRegression(), SVR(C=100), RandomForestRegressor(), MLPRegressor(max_iter=1000), XGBRegressor()]\n",
    "\n",
    "# Create lists for storing the evaluation metrics (RMSE, MAPE, R2) of each model \n",
    "rmse_ls = []\n",
    "mape_ls = []\n",
    "r2_ls = []\n",
    "\n",
    "# Loop through each model\n",
    "for model in models:\n",
    "    # Show model\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Scale numerical columns and encode categorical columns \n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"scaler\", StandardScaler(), numerical_columns),\n",
    "            (\"encoder\", OneHotEncoder(drop=None, sparse_output=False), categorical_columns)\n",
    "        ],\n",
    "        remainder=\"passthrough\"  # Include the boolean columns without transformation\n",
    "    )\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"column_transformer\", column_transformer),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the validation data\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "    # Calculate evaluation metrics: RMSE, MAPE, R2\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mape = mean_absolute_percentage_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    # Show evaluation metrics\n",
    "    print(f\"RMSE: {round(rmse, 2)}\")\n",
    "    print(f\"MAPE: {round(mape, 2)}\")\n",
    "    print(f\"R-squared (R²): {round(r2, 2)}\")\n",
    "\n",
    "    # Add evaluation metrics to their respective lists\n",
    "    rmse_ls.append(rmse)\n",
    "    mape_ls.append(mape)\n",
    "    r2_ls.append(r2)\n",
    "\n",
    "    # Create residual plots\n",
    "    plot_residuals(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482eca8-6853-44dc-ad83-7f4b4e5a31f8",
   "metadata": {},
   "source": [
    "# Classification Task  \n",
    "For a classification problem, where the goal is to predict a categorical target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd119a-14c4-46af-9e3e-facc98438ad0",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ae0de-8df6-4a3a-8c2e-404e167fe1ab",
   "metadata": {},
   "source": [
    "Data preprocessing steps:\n",
    "- Handled duplicates (e.g., by removing duplicates based on the ID column).\n",
    "- Handled incorrect data types (e.g., converted string columns to numerical columns where applicable).\n",
    "- Extracted features (e.g., created categorical, numerical, or boolean features from string columns).\n",
    "- Handled missing values (e.g., through imputation or deletion).\n",
    "\n",
    "Next Steps:\n",
    "- Split data into training, validation, and test sets (e.g., 70% train, 15% validation, 15% test).\n",
    "- Scale numerical features (e.g., using StandardScaler).\n",
    "- Encode categorical features (e.g., using One-Hot Encoding).\n",
    "- Train baseline models (e.g., regression, random forest, XGBoost).\n",
    "- Perform hyperparameter tuning (e.g., using GridSearchCV).\n",
    "- Select final model based on performance evaluation (e.g., using accuracy, precision, recall, F1 score, R-squared, or RMSE).\n",
    "- Save model weights (e.g., as a pickle file).\n",
    "- Save preprocessed data (e.g., as a `.csv` file)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-venv",
   "language": "python",
   "name": "machine-learning-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
